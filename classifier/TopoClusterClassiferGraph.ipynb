{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "molecular-lying",
   "metadata": {},
   "source": [
    "# Graph Networts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dated-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "received-witch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/home/users/mfong/anaconda3/envs/graph/lib/python3.8/site-packages/atlas_mpl_style/__init__.py:163: UserWarning: No LaTeX installation found -- atlas-mpl-style is falling back to usetex=False\n",
      "  _warn.warn(\n"
     ]
    }
   ],
   "source": [
    "#import libraries and some constants\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "import uproot3 as ur\n",
    "import atlas_mpl_style as ampl\n",
    "ampl.use_atlas_style()\n",
    "\n",
    "path_prefix = '/home/mfong/git/LCStudies/'\n",
    "plotpath = path_prefix + 'classifier/Plots/'\n",
    "modelpath = path_prefix + 'classifier/Models/'\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# metadata\n",
    "layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "len_phi = [4, 16, 16, 4, 4, 4]\n",
    "len_eta = [128, 16, 8, 4, 4, 2]\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "driven-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/home/users/mfong/anaconda3/envs/graph/lib/python3.8/site-packages/atlas_mpl_style/__init__.py:163: UserWarning: No LaTeX installation found -- atlas-mpl-style is falling back to usetex=False\n",
      "  _warn.warn(\n"
     ]
    }
   ],
   "source": [
    "# import our resolution utilities\n",
    "\n",
    "import sys\n",
    "sys.path.append(path_prefix)\n",
    "sys.path\n",
    "from util import resolution_util as ru\n",
    "from util import plot_util as pu\n",
    "from util import ml_util as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa305ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "answering-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:2', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:3', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=24220)]) #in MB\n",
    "\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "tf.config.list_physical_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "specialized-chancellor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pi0 events: 263891\n",
      "Number of pi+ events: 435967\n",
      "Number of pi- events: 434627\n",
      "Total: 1134485\n"
     ]
    }
   ],
   "source": [
    "# import pi+- vs. pi0 images\n",
    "\n",
    "inputpath = '/clusterfs/ml4hep/mfong/ML4Pions/v7/'    # ml4hep1 machine\n",
    "# inputpath = \"/data0/mfong/v7/\"    # voltan machine\n",
    "#path = '/eos/user/m/mswiatlo/images/'\n",
    "branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi', 'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_sumCellE', 'cluster_ENG_CALIB_TOT', 'cluster_ENG_CALIB_OUT_T', 'cluster_ENG_CALIB_DEAD_TOT', 'cluster_EM_PROBABILITY', 'cluster_HAD_WEIGHT', 'cluster_OOC_WEIGHT', 'cluster_DM_WEIGHT', 'cluster_CENTER_MAG', 'cluster_FIRST_ENG_DENS', 'cluster_cell_dR_min', 'cluster_cell_dR_max', 'cluster_cell_dEta_min', 'cluster_cell_dEta_max', 'cluster_cell_dPhi_min', 'cluster_cell_dPhi_max', 'cluster_cell_centerCellEta', 'cluster_cell_centerCellPhi', 'cluster_cell_centerCellLayer', 'cluster_cellE_norm']\n",
    "rootfiles = [\"pi0\", \"piplus\", \"piminus\"]\n",
    "trees = {\n",
    "    rfile : ur.open(inputpath+rfile+\".root\")['ClusterTree']\n",
    "    for rfile in rootfiles\n",
    "}\n",
    "pdata = {\n",
    "    ifile : itree.pandas.df(branches, flatten=False)\n",
    "    for ifile, itree in trees.items()\n",
    "}\n",
    "\n",
    "np0 = len(pdata['pi0'])\n",
    "npp = len(pdata['piplus'])\n",
    "npm = len(pdata['piminus'])\n",
    "\n",
    "print(\"Number of pi0 events: {}\".format(np0))\n",
    "print(\"Number of pi+ events: {}\".format(npp))\n",
    "print(\"Number of pi- events: {}\".format(npm))\n",
    "print(\"Total: {}\".format(np0+npp+npm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "known-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_shapes = {\n",
    "    'EMB1': (128,4),\n",
    "    'EMB2': (16,16),\n",
    "    'EMB3': (8,16),\n",
    "    'TileBar0': (4,4),\n",
    "    'TileBar1': (4,4),\n",
    "    'TileBar2': (2,4),\n",
    "}\n",
    "\n",
    "pcells = {\n",
    "    ifile : {\n",
    "        layer : mu.setupCells(itree, layer)\n",
    "        for layer in layers\n",
    "    }\n",
    "    for ifile, itree in trees.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "broken-loading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta', 'truthPhi',\n",
       "       'clusterIndex', 'nCluster', 'clusterE', 'clusterECalib', 'clusterPt',\n",
       "       'clusterEta', 'clusterPhi', 'cluster_nCells', 'cluster_sumCellE',\n",
       "       'cluster_ENG_CALIB_TOT', 'cluster_ENG_CALIB_OUT_T',\n",
       "       'cluster_ENG_CALIB_DEAD_TOT', 'cluster_EM_PROBABILITY',\n",
       "       'cluster_HAD_WEIGHT', 'cluster_OOC_WEIGHT', 'cluster_DM_WEIGHT',\n",
       "       'cluster_CENTER_MAG', 'cluster_FIRST_ENG_DENS', 'cluster_cell_dR_min',\n",
       "       'cluster_cell_dR_max', 'cluster_cell_dEta_min', 'cluster_cell_dEta_max',\n",
       "       'cluster_cell_dPhi_min', 'cluster_cell_dPhi_max',\n",
       "       'cluster_cell_centerCellEta', 'cluster_cell_centerCellPhi',\n",
       "       'cluster_cell_centerCellLayer', 'cluster_cellE_norm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata[\"pi0\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comprehensive-graham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263891, 512)\n",
      "(263891, 256)\n",
      "(263891, 128)\n",
      "(263891, 16)\n",
      "(263891, 16)\n",
      "(263891, 8)\n",
      "Total number of cells: 936\n"
     ]
    }
   ],
   "source": [
    "n_cells = 0\n",
    "for key in pcells[\"pi0\"]:\n",
    "    print(pcells[\"pi0\"][key].shape)\n",
    "    n_cells += pcells[\"pi0\"][key].shape[1]\n",
    "print(\"Total number of cells: \" + str(n_cells))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-blame",
   "metadata": {},
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vital-thompson",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df for pi0 only\n",
    "df_p0 = pd.DataFrame(np.concatenate([pcells[\"pi0\"][key] for key in pcells[\"pi0\"].keys()], axis = 1))\n",
    "\n",
    "col_names = []\n",
    "for key in pcells[\"pi0\"].keys():\n",
    "    col_names.extend([key + \"_\" + str(i) for i in range(len(pcells[\"pi0\"][key][0]))])\n",
    "df_p0.columns = col_names\n",
    "\n",
    "df_p0[\"is_p0\"] = 1\n",
    "\n",
    "\n",
    "# print(df_p0.shape)\n",
    "# df_p0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rough-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df for pipplus and piminus\n",
    "df_pp = pd.DataFrame(np.concatenate([pcells[\"piplus\"][key] for key in pcells[\"piplus\"].keys()], axis = 1))\n",
    "df_pp.columns = col_names\n",
    "df_pp[\"is_p0\"] = 0\n",
    "\n",
    "df_pm = pd.DataFrame(np.concatenate([pcells[\"piminus\"][key] for key in pcells[\"piminus\"].keys()], axis = 1))\n",
    "df_pm.columns = col_names\n",
    "df_pm[\"is_p0\"] = 0\n",
    "\n",
    "# print(df_pp.shape)\n",
    "# df_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "virgin-emission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMB1_0</th>\n",
       "      <th>EMB1_1</th>\n",
       "      <th>EMB1_2</th>\n",
       "      <th>EMB1_3</th>\n",
       "      <th>EMB1_4</th>\n",
       "      <th>EMB1_5</th>\n",
       "      <th>EMB1_6</th>\n",
       "      <th>EMB1_7</th>\n",
       "      <th>EMB1_8</th>\n",
       "      <th>EMB1_9</th>\n",
       "      <th>EMB1_10</th>\n",
       "      <th>EMB1_11</th>\n",
       "      <th>EMB1_12</th>\n",
       "      <th>EMB1_13</th>\n",
       "      <th>EMB1_14</th>\n",
       "      <th>EMB1_15</th>\n",
       "      <th>EMB1_16</th>\n",
       "      <th>EMB1_17</th>\n",
       "      <th>EMB1_18</th>\n",
       "      <th>EMB1_19</th>\n",
       "      <th>EMB1_20</th>\n",
       "      <th>EMB1_21</th>\n",
       "      <th>EMB1_22</th>\n",
       "      <th>EMB1_23</th>\n",
       "      <th>EMB1_24</th>\n",
       "      <th>EMB1_25</th>\n",
       "      <th>EMB1_26</th>\n",
       "      <th>EMB1_27</th>\n",
       "      <th>EMB1_28</th>\n",
       "      <th>EMB1_29</th>\n",
       "      <th>EMB1_30</th>\n",
       "      <th>EMB1_31</th>\n",
       "      <th>EMB1_32</th>\n",
       "      <th>EMB1_33</th>\n",
       "      <th>EMB1_34</th>\n",
       "      <th>EMB1_35</th>\n",
       "      <th>EMB1_36</th>\n",
       "      <th>EMB1_37</th>\n",
       "      <th>EMB1_38</th>\n",
       "      <th>EMB1_39</th>\n",
       "      <th>EMB1_40</th>\n",
       "      <th>EMB1_41</th>\n",
       "      <th>EMB1_42</th>\n",
       "      <th>EMB1_43</th>\n",
       "      <th>EMB1_44</th>\n",
       "      <th>EMB1_45</th>\n",
       "      <th>EMB1_46</th>\n",
       "      <th>EMB1_47</th>\n",
       "      <th>EMB1_48</th>\n",
       "      <th>EMB1_49</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB3_119</th>\n",
       "      <th>EMB3_120</th>\n",
       "      <th>EMB3_121</th>\n",
       "      <th>EMB3_122</th>\n",
       "      <th>EMB3_123</th>\n",
       "      <th>EMB3_124</th>\n",
       "      <th>EMB3_125</th>\n",
       "      <th>EMB3_126</th>\n",
       "      <th>EMB3_127</th>\n",
       "      <th>TileBar0_0</th>\n",
       "      <th>TileBar0_1</th>\n",
       "      <th>TileBar0_2</th>\n",
       "      <th>TileBar0_3</th>\n",
       "      <th>TileBar0_4</th>\n",
       "      <th>TileBar0_5</th>\n",
       "      <th>TileBar0_6</th>\n",
       "      <th>TileBar0_7</th>\n",
       "      <th>TileBar0_8</th>\n",
       "      <th>TileBar0_9</th>\n",
       "      <th>TileBar0_10</th>\n",
       "      <th>TileBar0_11</th>\n",
       "      <th>TileBar0_12</th>\n",
       "      <th>TileBar0_13</th>\n",
       "      <th>TileBar0_14</th>\n",
       "      <th>TileBar0_15</th>\n",
       "      <th>TileBar1_0</th>\n",
       "      <th>TileBar1_1</th>\n",
       "      <th>TileBar1_2</th>\n",
       "      <th>TileBar1_3</th>\n",
       "      <th>TileBar1_4</th>\n",
       "      <th>TileBar1_5</th>\n",
       "      <th>TileBar1_6</th>\n",
       "      <th>TileBar1_7</th>\n",
       "      <th>TileBar1_8</th>\n",
       "      <th>TileBar1_9</th>\n",
       "      <th>TileBar1_10</th>\n",
       "      <th>TileBar1_11</th>\n",
       "      <th>TileBar1_12</th>\n",
       "      <th>TileBar1_13</th>\n",
       "      <th>TileBar1_14</th>\n",
       "      <th>TileBar1_15</th>\n",
       "      <th>TileBar2_0</th>\n",
       "      <th>TileBar2_1</th>\n",
       "      <th>TileBar2_2</th>\n",
       "      <th>TileBar2_3</th>\n",
       "      <th>TileBar2_4</th>\n",
       "      <th>TileBar2_5</th>\n",
       "      <th>TileBar2_6</th>\n",
       "      <th>TileBar2_7</th>\n",
       "      <th>is_p0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.070766</td>\n",
       "      <td>0.140632</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.006990</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.047344</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.006787</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011872</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.088836</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030488</td>\n",
       "      <td>0.019321</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019718</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087662</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006445</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 937 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMB1_0  EMB1_1  EMB1_2  EMB1_3  EMB1_4  EMB1_5  EMB1_6  EMB1_7  EMB1_8  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   EMB1_9  EMB1_10  EMB1_11  EMB1_12  EMB1_13  EMB1_14  EMB1_15  EMB1_16  \\\n",
       "0     0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1     0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2     0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3     0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4     0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   EMB1_17  EMB1_18  EMB1_19  EMB1_20  EMB1_21  EMB1_22  EMB1_23  EMB1_24  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   EMB1_25  EMB1_26  EMB1_27  EMB1_28  EMB1_29  EMB1_30  EMB1_31  EMB1_32  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   EMB1_33  EMB1_34  EMB1_35  EMB1_36  EMB1_37  EMB1_38  EMB1_39  EMB1_40  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   EMB1_41  EMB1_42  EMB1_43  EMB1_44  EMB1_45  EMB1_46  EMB1_47  EMB1_48  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   EMB1_49  ...  EMB3_119  EMB3_120  EMB3_121  EMB3_122  EMB3_123  EMB3_124  \\\n",
       "0      0.0  ...  0.000009  0.000038  0.000009       0.0       0.0       0.0   \n",
       "1      0.0  ...  0.000000  0.000000  0.000000       0.0       0.0       0.0   \n",
       "2      0.0  ...  0.000000  0.000000  0.000000       0.0       0.0       0.0   \n",
       "3      0.0  ...  0.000000  0.000000  0.000000       0.0       0.0       0.0   \n",
       "4      0.0  ...  0.000000  0.000000  0.000000       0.0       0.0       0.0   \n",
       "\n",
       "   EMB3_125  EMB3_126  EMB3_127  TileBar0_0  TileBar0_1  TileBar0_2  \\\n",
       "0       0.0       0.0       0.0    0.000224    0.000315    0.001344   \n",
       "1       0.0       0.0       0.0    0.000000    0.000000    0.000000   \n",
       "2       0.0       0.0       0.0    0.000000    0.011872    0.005269   \n",
       "3       0.0       0.0       0.0    0.000000    0.000000    0.000000   \n",
       "4       0.0       0.0       0.0    0.000000    0.000000    0.000000   \n",
       "\n",
       "   TileBar0_3  TileBar0_4  TileBar0_5  TileBar0_6  TileBar0_7  TileBar0_8  \\\n",
       "0    0.000428    0.002322    0.012944    0.014262    0.002498    0.001551   \n",
       "1    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "2    0.000000    0.000000    0.628931    0.088836    0.002091    0.000000   \n",
       "3    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "4    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "   TileBar0_9  TileBar0_10  TileBar0_11  TileBar0_12  TileBar0_13  \\\n",
       "0    0.070766     0.140632     0.001669     0.003287     0.004747   \n",
       "1    0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "2    0.030488     0.019321     0.004342     0.000000     0.000000   \n",
       "3    0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "4    0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "   TileBar0_14  TileBar0_15  TileBar1_0  TileBar1_1  TileBar1_2  TileBar1_3  \\\n",
       "0     0.008165     0.000888    0.000054    0.000776    0.001186    0.000146   \n",
       "1     0.000000     0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "2     0.000177     0.001947    0.000000    0.019718    0.003629    0.000000   \n",
       "3     0.000000     0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "4     0.000000     0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "   TileBar1_4  TileBar1_5  TileBar1_6  TileBar1_7  TileBar1_8  TileBar1_9  \\\n",
       "0    0.000375    0.006990    0.007225    0.003866    0.001419    0.035971   \n",
       "1    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "2    0.000000    0.087662    0.022248    0.000901    0.000000    0.006445   \n",
       "3    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "4    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "   TileBar1_10  TileBar1_11  TileBar1_12  TileBar1_13  TileBar1_14  \\\n",
       "0     0.047344     0.003773     0.000361     0.006787     0.003358   \n",
       "1     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "2     0.000219     0.000237     0.000000     0.000000     0.000000   \n",
       "3     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "4     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "   TileBar1_15  TileBar2_0  TileBar2_1  TileBar2_2  TileBar2_3  TileBar2_4  \\\n",
       "0     0.002189         0.0    0.000625    0.000405    0.000031    0.000024   \n",
       "1     0.000000         0.0    0.000000    0.000000    0.000000    0.000000   \n",
       "2     0.000000         0.0    0.000000    0.000000    0.000000    0.000000   \n",
       "3     0.000000         0.0    0.000000    0.000000    0.000000    0.000000   \n",
       "4     0.000000         0.0    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "   TileBar2_5  TileBar2_6  TileBar2_7  is_p0  \n",
       "0    0.001023    0.001606         0.0      0  \n",
       "1    0.000000    0.000000         0.0      0  \n",
       "2    0.000000    0.000000         0.0      0  \n",
       "3    0.000000    0.000000         0.0      0  \n",
       "4    0.000000    0.000000         0.0      1  \n",
       "\n",
       "[5 rows x 937 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create final df\n",
    "df = df_p0.append(df_pp.append(df_pm))\n",
    "df = df.sample(frac=1) # Shuffle the df so pi0 are not all first\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-hanging",
   "metadata": {},
   "source": [
    "## Create Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "facial-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutations for doubly connected edges\n",
    "from itertools import permutations\n",
    "import functools\n",
    "import networkx as nx\n",
    "import sonnet as snt\n",
    "\n",
    "from graph_nets import blocks\n",
    "\n",
    "from graph_nets import graphs\n",
    "from graph_nets import modules\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "banned-scottish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMB1_0        0.000000\n",
       "EMB1_1        0.000000\n",
       "EMB1_2        0.000000\n",
       "EMB1_3        0.000000\n",
       "EMB1_4        0.000000\n",
       "                ...   \n",
       "TileBar2_4    0.000024\n",
       "TileBar2_5    0.001023\n",
       "TileBar2_6    0.001606\n",
       "TileBar2_7    0.000000\n",
       "is_p0         0.000000\n",
       "Name: 0, Length: 937, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event0 = df.loc[0]\n",
    "event0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "appropriate-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fully_connected_edges(nodes):\n",
    "    \"\"\"\n",
    "    returns a list of tuples with (sender_node, reciever_node) for a fully connected graph\n",
    "    ex: [(1,2), (2,1), (0,1)]\n",
    "    \"\"\"\n",
    "    n_nodes = len(nodes)\n",
    "    return list(permutations(range(n_nodes), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ceramic-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(event):\n",
    "    \n",
    "    n_nodes = 0\n",
    "    nodes = []\n",
    "    MIN_VALUE = 0.05\n",
    "    solution = \"is_p0\"\n",
    "    \n",
    "    nodes = [[cell] for cell in event[col_names][event[col_names] > MIN_VALUE]]\n",
    "    n_nodes = len(nodes)\n",
    "    if n_nodes < 1:\n",
    "        return (None, None)\n",
    "    nodes = np.array(nodes, dtype=np.float32)\n",
    "    \n",
    "    edge_endpoints = make_fully_connected_edges(nodes)\n",
    "    senders = np.array([x[0] for x in edge_endpoints])\n",
    "    receivers = np.array([x[1] for x in edge_endpoints])\n",
    "    n_edges = len(edge_endpoints)\n",
    "    edges = np.expand_dims(np.array([0.0]*n_edges, dtype=np.float32), axis=1)\n",
    "\n",
    "    \n",
    "    input_datadict = {\n",
    "        \"n_node\": n_nodes,\n",
    "        \"n_edge\": n_edges,\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges,\n",
    "        \"senders\": senders,\n",
    "        \"receivers\": receivers,\n",
    "        \"globals\": np.array([n_nodes], dtype=np.float32)\n",
    "    }\n",
    "    target_datadict = {\n",
    "        \"n_node\": n_nodes,\n",
    "        \"n_edge\": n_edges,\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges,\n",
    "        \"senders\": senders,\n",
    "        \"receivers\": receivers,\n",
    "        \"globals\": np.array([event[solution]], dtype=np.float32)\n",
    "    }\n",
    "    input_graph = utils_tf.data_dicts_to_graphs_tuple([input_datadict])\n",
    "    target_graph = utils_tf.data_dicts_to_graphs_tuple([target_datadict])\n",
    "    \n",
    "    return (input_graph, target_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fatty-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graphs_tuple(g, data=True):\n",
    "    for field_name in graphs.ALL_FIELDS:\n",
    "        per_replica_sample = getattr(g, field_name)\n",
    "        if per_replica_sample is None:\n",
    "            print(field_name, \"EMPTY\")\n",
    "        else:\n",
    "            print(field_name, \"has shape\", per_replica_sample.shape)\n",
    "            if data and  field_name != \"edges\":\n",
    "                print(per_replica_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "characteristic-editing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes has shape (4, 1)\n",
      "edges has shape (12, 1)\n",
      "receivers has shape (12,)\n",
      "senders has shape (12,)\n",
      "globals has shape (1, 1)\n",
      "n_node has shape (1,)\n",
      "n_edge has shape (1,)\n"
     ]
    }
   ],
   "source": [
    "graphs_tuple0_input, graphs_tuple0_target = make_graph(event0)\n",
    "\n",
    "print_graphs_tuple(graphs_tuple0_input, data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "improving-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "for i in range(100):\n",
    "    graphs.append(make_graph(df.loc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "macro-cardiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([30], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([56], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([30], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([42], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([42], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([30], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([56], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([42], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([20], shape=(1,), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor([6], shape=(1,), dtype=int32)\n",
      "tf.Tensor([30], shape=(1,), dtype=int32)\n",
      "tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "tf.Tensor([56], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# [x[0] for x in graphs][0].n_node\n",
    "\n",
    "for test_input, _ in graphs:\n",
    "    if test_input is not None:\n",
    "        print(test_input.n_edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "structural-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting functions from example (broken)\n",
    "\n",
    "# def plot_graph_networkx(graph, ax, pos=None):\n",
    "#   node_labels = {node: \"{:.3g}\".format(data[\"features\"][0])\n",
    "#                  for node, data in graph.nodes(data=True)\n",
    "#                  if data[\"features\"] is not None}\n",
    "#   edge_labels = {(sender, receiver): \"{:.3g}\".format(data[\"features\"][0])\n",
    "#                  for sender, receiver, data in graph.edges(data=True)\n",
    "#                  if data[\"features\"] is not None}\n",
    "#   global_label = (\"{:.3g}\".format(graph.graph[\"features\"][0])\n",
    "#                   if graph.graph[\"features\"] is not None else None)\n",
    "\n",
    "#   if pos is None:\n",
    "#     pos = nx.spring_layout(graph)\n",
    "#   nx.draw_networkx(graph, pos, ax=ax, labels=node_labels)\n",
    "\n",
    "#   if edge_labels:\n",
    "#     nx.draw_networkx_edge_labels(graph, pos, edge_labels, ax=ax)\n",
    "\n",
    "#   if global_label:\n",
    "#     plt.text(0.05, 0.95, global_label, transform=ax.transAxes)\n",
    "\n",
    "#   ax.yaxis.set_visible(False)\n",
    "#   ax.xaxis.set_visible(False)\n",
    "#   return pos\n",
    "\n",
    "# def plot_graphs_tuple(graphs_tuple):\n",
    "#   networkx_graphs = utils_np.graphs_tuple_to_networkxs(graphs_tuple)\n",
    "#   num_graphs = len(networkx_graphs)\n",
    "#   _, axes = plt.subplots(1, num_graphs, figsize=(5*num_graphs, 5))\n",
    "#   if num_graphs == 1:\n",
    "#     axes = axes,\n",
    "#   for graph, ax in zip(networkx_graphs, axes):\n",
    "#     plot_graph_networkx(graph, ax)\n",
    "\n",
    "# plot_graphs_tuple(graphs_tuple0_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-closer",
   "metadata": {},
   "source": [
    "## Graph net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "apparent-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the newest dev version of graph_nets (see https://github.com/deepmind/graph_nets/issues/139)\n",
    "# as of 3/25/2021\n",
    "\n",
    "\n",
    "# !pip install git+git://github.com/deepmind/graph_nets.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "local-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2\n",
    "def make_mlp_model():\n",
    "  \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "\n",
    "  The parameters of each new MLP are not shared with others generated by\n",
    "  this function.\n",
    "\n",
    "  Returns:\n",
    "    A Sonnet module which contains the MLP and LayerNorm.\n",
    "  \"\"\"\n",
    "  # the activation function choices:\n",
    "  # swish, relu, relu6, leaky_relu\n",
    "  return snt.Sequential([\n",
    "      snt.nets.MLP([128, 64]*NUM_LAYERS,\n",
    "                    activation=tf.nn.relu,\n",
    "                    activate_final=True, \n",
    "                  #  dropout_rate=DROPOUT_RATE\n",
    "        ),\n",
    "      snt.LayerNorm(axis=-1, create_scale=True, create_offset=False)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "composed-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGraphNetwork(snt.Module):\n",
    "    \"\"\"GraphIndependent with MLP edge, node, and global models.\"\"\"\n",
    "    def __init__(self, name=\"MLPGraphNetwork\"):\n",
    "        super(MLPGraphNetwork, self).__init__(name=name)\n",
    "        self._network = modules.GraphNetwork(\n",
    "            edge_model_fn=make_mlp_model,\n",
    "            node_model_fn=make_mlp_model,\n",
    "            global_model_fn=make_mlp_model\n",
    "            )\n",
    "\n",
    "    def __call__(self, inputs,\n",
    "            edge_model_kwargs=None,\n",
    "            node_model_kwargs=None,\n",
    "            global_model_kwargs=None):\n",
    "        return self._network(inputs,\n",
    "                      edge_model_kwargs=edge_model_kwargs,\n",
    "                      node_model_kwargs=node_model_kwargs,\n",
    "                      global_model_kwargs=global_model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "handed-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_SIZE = 64\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "class GlobalClassifierNoEdgeInfo(snt.Module):\n",
    "\n",
    "    def __init__(self, name=\"GlobalClassifierNoEdgeInfo\"):\n",
    "        super(GlobalClassifierNoEdgeInfo, self).__init__(name=name)\n",
    "\n",
    "        self._edge_block = blocks.EdgeBlock(\n",
    "            edge_model_fn=make_mlp_model,\n",
    "            use_edges=True,\n",
    "            use_receiver_nodes=True,\n",
    "            use_sender_nodes=True,\n",
    "            use_globals=True,\n",
    "            name='edge_encoder_block')\n",
    "\n",
    "        self._node_encoder_block = blocks.NodeBlock(\n",
    "            node_model_fn=make_mlp_model,\n",
    "            use_received_edges=True,\n",
    "            use_sent_edges=True,\n",
    "            use_nodes=True,\n",
    "            use_globals=True,\n",
    "            name='node_encoder_block'\n",
    "        )\n",
    "\n",
    "        self._global_block = blocks.GlobalBlock(\n",
    "            global_model_fn=make_mlp_model,\n",
    "            use_edges=True,\n",
    "            use_nodes=True,\n",
    "            use_globals=True,\n",
    "        )\n",
    "\n",
    "        self._core = MLPGraphNetwork()\n",
    "        # Transforms the outputs into appropriate shapes.\n",
    "        global_output_size = 1\n",
    "        global_fn =lambda: snt.Sequential([\n",
    "            snt.nets.MLP([LATENT_SIZE, global_output_size] * NUM_LAYERS, name='global_output'),\n",
    "            tf.sigmoid\n",
    "        ])\n",
    "\n",
    "        self._output_transform = modules.GraphIndependent(None, None, global_fn)\n",
    "\n",
    "    def __call__(self, input_op, num_processing_steps):\n",
    "        latent = self._global_block(self._edge_block(self._node_encoder_block(input_op)))\n",
    "        latent0 = latent\n",
    "\n",
    "        output_ops = []\n",
    "        for _ in range(num_processing_steps):\n",
    "            core_input = utils_tf.concat([latent0, latent], axis=1)\n",
    "            latent = self._core(core_input)\n",
    "            output_ops.append(self._output_transform(latent))\n",
    "\n",
    "        return output_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "removed-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GlobalClassifierNoEdgeInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "compact-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_graphs = model(graphs_tuple0_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cosmetic-valuable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.23835541]], dtype=float32)>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.globals for x in output_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "solved-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function:\n",
    "\n",
    "class GlobalLoss:\n",
    "    def __init__(self, real_global_weight, fake_global_weight):\n",
    "        self.w_global_real = real_global_weight\n",
    "        self.w_global_fake = fake_global_weight\n",
    "\n",
    "    def __call__(self, target_op, output_ops):\n",
    "        global_weights = target_op.globals * self.w_global_real \\\n",
    "            + (1 - target_op.globals) * self.w_global_fake\n",
    "        \n",
    "        print(global_weights)\n",
    "        \n",
    "        loss_ops = [\n",
    "            tf.compat.v1.losses.log_loss(target_op.globals, output_op.globals, weights=global_weights)\n",
    "            for output_op in output_ops\n",
    "        ]\n",
    "        return tf.stack(loss_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "mexican-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_global = GlobalLoss(real_global_weight = 1.0, fake_global_weight = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "offshore-economy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.69314694], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_global(graphs_tuple0_target, output_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "missing-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = []\n",
    "target_list = []\n",
    "counter = 0\n",
    "index = 0\n",
    "# while counter < 100:\n",
    "#     input_graph, target_graph = train_graphs[index]\n",
    "#     if input_graph is None:\n",
    "#         index += 1\n",
    "#         continue\n",
    "#     target_list.append(target_graph)\n",
    "#     input_list.append(input_graph)\n",
    "#     counter += 1\n",
    "\n",
    "# concated_inputs = utils_tf.concat(input_list, axis=0)\n",
    "# concated_targets = utils_tf.concat(target_graph, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_list = []\n",
    "for data in train_graphs[:1000]:\n",
    "    input_tr, target_tr = data\n",
    "    if input_tr is None:\n",
    "            continue\n",
    "    input_list.append(input_tr)\n",
    "    target_list.append(target_tr)\n",
    "    if len(input_list) >= batch_size:\n",
    "        input_tr = utils_tf.concat(input_list, axis=0)\n",
    "        target_tr = utils_tf.concat(target_list, axis=0)\n",
    "        x = model(input_tr, 1)\n",
    "        break\n",
    "    input_list = []\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "alpha-spring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(979, 1), dtype=float32, numpy=\n",
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]], dtype=float32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils_tf.concat(target_list, axis=0).globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fantastic-compatibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.23835541]], dtype=float32)>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_tr, 1)[0].globals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-cherry",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "noted-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO modify acc function\n",
    "\n",
    "# def compute_accuracy(target, output):\n",
    "#     \"\"\"Calculate model accuracy.\n",
    "\n",
    "#     Returns the number of correctly predicted links and the number\n",
    "#     of completely solved list sorts (100% correct predictions).\n",
    "\n",
    "#     Args:\n",
    "#     target: A `graphs.GraphsTuple` that contains the target graph.\n",
    "#     output: A `graphs.GraphsTuple` that contains the output graph.\n",
    "\n",
    "#     Returns:\n",
    "#     correct: A `float` fraction of correctly labeled nodes/edges.\n",
    "#     solved: A `float` fraction of graphs that are completely correctly labeled.\n",
    "#     \"\"\"\n",
    "#     tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "#     odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "#     cs = []\n",
    "#     ss = []\n",
    "#     for td, od in zip(tdds, odds):\n",
    "#         num_elements = td[\"nodes\"].shape[0]\n",
    "#         xn = np.argmax(td[\"nodes\"], axis=-1)\n",
    "#         yn = np.argmax(od[\"nodes\"], axis=-1)\n",
    "\n",
    "#         xe = np.reshape(\n",
    "#             np.argmax(\n",
    "#                 np.reshape(td[\"edges\"], (num_elements, num_elements, 2)), axis=-1),\n",
    "#             (-1,))\n",
    "#         ye = np.reshape(\n",
    "#             np.argmax(\n",
    "#                 np.reshape(od[\"edges\"], (num_elements, num_elements, 2)), axis=-1),\n",
    "#             (-1,))\n",
    "#         c = np.concatenate((xn == yn, xe == ye), axis=0)\n",
    "#         s = np.all(c)\n",
    "#         cs.append(c)\n",
    "#         ss.append(s)\n",
    "#     correct = np.mean(np.concatenate(cs, axis=0))\n",
    "#     solved = np.mean(np.stack(ss))\n",
    "#     return correct, solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cloudy-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature(dataset, batch_size):\n",
    "    \"\"\"\n",
    "    Get signature of inputs for the training loop.\n",
    "    The signature is used by the tf.function\n",
    "    \"\"\"\n",
    "\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    for _, data in dataset.iterrows():\n",
    "        dd = make_graph(data)\n",
    "        if dd[0] is not None:\n",
    "            input_list.append(dd[0])\n",
    "            target_list.append(dd[1])\n",
    "            \n",
    "        if len(input_list) == batch_size:\n",
    "            break\n",
    "\n",
    "    inputs = utils_tf.concat(input_list, axis=0)\n",
    "    targets = utils_tf.concat(target_list, axis=0)\n",
    "    input_signature = (\n",
    "      utils_tf.specs_from_graphs_tuple(inputs),\n",
    "      utils_tf.specs_from_graphs_tuple(targets)\n",
    "    )\n",
    "    \n",
    "    return input_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "several-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "input_signature = get_signature(df, batch_size)\n",
    "\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps_tr = 10\n",
    "num_processing_steps_ge = 10\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = snt.optimizers.Adam(learning_rate)\n",
    "\n",
    "\n",
    "# model = models.EncodeProcessDecode(edge_output_size=2, node_output_size=2)\n",
    "last_iteration = 0\n",
    "generalization_iteration = 0\n",
    "\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []\n",
    "\n",
    "\n",
    "@functools.partial(tf.function, input_signature=input_signature)\n",
    "def update_step(inputs_tr, targets_tr):\n",
    "    print(\"Tracing update_step\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs_tr = model(inputs_tr, num_processing_steps_tr)\n",
    "        loss_ops_tr = loss_function_global(targets_tr, outputs_tr)\n",
    "        loss_op_tr = tf.math.reduce_sum(loss_ops_tr) / tf.constant(num_processing_steps_tr, dtype=tf.float32)\n",
    "\n",
    "    gradients = tape.gradient(loss_op_tr, model.trainable_variables)\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    return outputs_tr, loss_op_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "stupid-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train and generalization df\n",
    "df_train, df_test = train_test_split(df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "square-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize values to gaussian\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# fit scaler to train set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train.drop(\"is_p0\", axis=1))\n",
    "\n",
    "# scale the train set\n",
    "df_train_scaled = pd.DataFrame(scaler.transform(df_train.drop(\"is_p0\", axis=1)))\n",
    "df_train_scaled[\"is_p0\"] = df_train[\"is_p0\"].values\n",
    "df_train_scaled.columns = df_train.columns\n",
    "\n",
    "# scale the test set\n",
    "df_test_scaled = pd.DataFrame(scaler.transform(df_test.drop(\"is_p0\", axis=1)))\n",
    "df_test_scaled[\"is_p0\"] = df_test[\"is_p0\"].values\n",
    "df_test_scaled.columns = df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "figured-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 40s, sys: 14.6 s, total: 5min 55s\n",
      "Wall time: 4min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO this is very slow (1.5 hours for full 1100000 row dataset)\n",
    "# 5 min for 50000 rows\n",
    "# make graphs for each event\n",
    "train_graphs = [make_graph(event) for _, event in df_train_scaled[:50000].iterrows()]\n",
    "test_graphs = [make_graph(event) for _, event in df_test_scaled[:5000].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save train_graphs and test_graphs objects to file, it takes too long to make\n",
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, \"wb\") as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_object(train_graphs, \"Temp/train_graphs_scaled.pkl\")\n",
    "save_object(train_graphs, \"Temp/test_graphs_scaled.pkl\")\n",
    "\n",
    "\"\"\"\n",
    "Load the graph dataset from pickle object files\n",
    "\"\"\"\n",
    "# file = open(\"Temp/train_graphs.pkl\",'rb')\n",
    "# train_graphs = pickle.load(file)\n",
    "# file.close()\n",
    "\n",
    "# file = open(\"Temp/test_graphs.pkl\",'rb')\n",
    "# test_graphs = pickle.load(file)\n",
    "# file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "instant-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_dataset(datasets, batch_size):\n",
    "    if batch_size > 0:\n",
    "        in_list = []\n",
    "        target_list = []\n",
    "        for dataset in datasets:\n",
    "            inputs_tr, targets_tr = dataset\n",
    "            if inputs_tr is None:\n",
    "                continue\n",
    "            in_list.append(inputs_tr)\n",
    "            target_list.append(targets_tr)\n",
    "            if len(in_list) == batch_size:\n",
    "                inputs_tr = utils_tf.concat(in_list, axis=0)\n",
    "                targets_tr = utils_tf.concat(target_list, axis=0)\n",
    "                yield (inputs_tr, targets_tr)\n",
    "                in_list = []\n",
    "                target_list = []\n",
    "    else:\n",
    "        for dataset in datasets:\n",
    "            if dataset is None:\n",
    "                continue\n",
    "            yield dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "coordinated-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator that yields a graph of the batch_size concatenated graphs\n",
    "training_data = loop_dataset(train_graphs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "strategic-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tr, target_tr = next(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dated-rental",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907588"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "damaged-session",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4610453248023987\n",
      "Loss: 0.46114158630371094\n",
      "Loss: 0.4609955847263336\n",
      "Loss: 0.46059614419937134\n",
      "Loss: 0.4599519670009613\n",
      "Loss: 0.45918089151382446\n",
      "Loss: 0.4583708941936493\n",
      "Loss: 0.4576148986816406\n",
      "Loss: 0.45698490738868713\n",
      "Loss: 0.45671215653419495\n",
      "Loss: 0.4567072093486786\n",
      "Loss: 0.45671185851097107\n",
      "Loss: 0.4566919505596161\n",
      "Loss: 0.4566284120082855\n",
      "Loss: 0.45650988817214966\n",
      "Loss: 0.45633697509765625\n",
      "Loss: 0.4561152458190918\n",
      "Loss: 0.4558555781841278\n",
      "Loss: 0.455574095249176\n",
      "Loss: 0.4552818238735199\n",
      "Loss: 0.45498958230018616\n",
      "Loss: 0.45475897192955017\n",
      "Loss: 0.45455238223075867\n",
      "Loss: 0.45436397194862366\n",
      "Loss: 0.45421481132507324\n",
      "Loss: 0.45408591628074646\n",
      "Loss: 0.45397621393203735\n",
      "Loss: 0.45388126373291016\n",
      "Loss: 0.45382919907569885\n",
      "Loss: 0.4537911117076874\n",
      "Loss: 0.4537179172039032\n",
      "Loss: 0.45358729362487793\n",
      "Loss: 0.45342347025871277\n",
      "Loss: 0.4533112049102783\n",
      "Loss: 0.4532068371772766\n",
      "Loss: 0.4531041085720062\n",
      "Loss: 0.45301198959350586\n",
      "Loss: 0.45291003584861755\n",
      "Loss: 0.4528124928474426\n",
      "Loss: 0.45271894335746765\n",
      "Loss: 0.4526293873786926\n",
      "Loss: 0.45255133509635925\n",
      "Loss: 0.45248839259147644\n",
      "Loss: 0.45242634415626526\n",
      "Loss: 0.4523608386516571\n",
      "Loss: 0.4522974193096161\n",
      "Loss: 0.4522411525249481\n",
      "Loss: 0.452169269323349\n",
      "Loss: 0.45209869742393494\n",
      "Loss: 0.45203256607055664\n",
      "Loss: 0.45197102427482605\n",
      "Loss: 0.45190855860710144\n",
      "Loss: 0.4518461227416992\n",
      "Loss: 0.4517941474914551\n",
      "Loss: 0.45174726843833923\n",
      "Loss: 0.45170798897743225\n",
      "Loss: 0.4516640603542328\n",
      "Loss: 0.45161229372024536\n",
      "Loss: 0.45157313346862793\n",
      "Loss: 0.45153409242630005\n",
      "Loss: 0.4514945149421692\n",
      "Loss: 0.4514525532722473\n",
      "Loss: 0.451416939496994\n",
      "Loss: 0.45137983560562134\n",
      "Loss: 0.45134636759757996\n",
      "Loss: 0.4513120651245117\n",
      "Loss: 0.4512798488140106\n",
      "Loss: 0.45125246047973633\n",
      "Loss: 0.4512215554714203\n",
      "Loss: 0.4511856138706207\n",
      "Loss: 0.45115190744400024\n",
      "Loss: 0.4511263072490692\n",
      "Loss: 0.45109835267066956\n",
      "Loss: 0.45107150077819824\n",
      "Loss: 0.4510463774204254\n",
      "Loss: 0.45101800560951233\n",
      "Loss: 0.45098814368247986\n",
      "Loss: 0.450965017080307\n",
      "Loss: 0.4509299397468567\n",
      "Loss: 0.4509093463420868\n",
      "Loss: 0.4508797824382782\n",
      "Loss: 0.450851172208786\n",
      "Loss: 0.45081692934036255\n",
      "Loss: 0.45078951120376587\n",
      "Loss: 0.4507468342781067\n",
      "Loss: 0.45070719718933105\n",
      "Loss: 0.4506552219390869\n",
      "Loss: 0.4505864083766937\n",
      "Loss: 0.4504864811897278\n",
      "Loss: 0.450423926115036\n",
      "Loss: 0.4504179060459137\n",
      "Loss: 0.4502590298652649\n",
      "Loss: 0.4502054750919342\n",
      "Loss: 0.45019206404685974\n",
      "Loss: 0.4501475393772125\n",
      "Loss: 0.4500628113746643\n",
      "Loss: 0.45004016160964966\n",
      "Loss: 0.45009586215019226\n",
      "Loss: 0.45011910796165466\n",
      "Loss: 0.45010051131248474\n",
      "Loss: 0.4500119686126709\n",
      "Loss: 0.44991984963417053\n",
      "Loss: 0.44987136125564575\n",
      "Loss: 0.4498938024044037\n",
      "Loss: 0.4498666822910309\n",
      "Loss: 0.4498121738433838\n",
      "Loss: 0.4497489035129547\n",
      "Loss: 0.44970592856407166\n",
      "Loss: 0.4496884047985077\n",
      "Loss: 0.44968166947364807\n",
      "Loss: 0.4496472477912903\n",
      "Loss: 0.4495096802711487\n",
      "Loss: 0.44964519143104553\n",
      "Loss: 0.44953233003616333\n",
      "Loss: 0.4495117664337158\n",
      "Loss: 0.4496486783027649\n",
      "Loss: 0.44955530762672424\n",
      "Loss: 0.4495149552822113\n",
      "Loss: 0.44951310753822327\n",
      "Loss: 0.4494757354259491\n",
      "Loss: 0.44940948486328125\n",
      "Loss: 0.4494658410549164\n",
      "Loss: 0.44945716857910156\n",
      "Loss: 0.44939857721328735\n",
      "Loss: 0.44930917024612427\n",
      "Loss: 0.44931650161743164\n",
      "Loss: 0.44926711916923523\n",
      "Loss: 0.44926711916923523\n",
      "Loss: 0.44923868775367737\n",
      "Loss: 0.4491632580757141\n",
      "Loss: 0.44913506507873535\n",
      "Loss: 0.4490969181060791\n",
      "Loss: 0.4490525722503662\n",
      "Loss: 0.44903650879859924\n",
      "Loss: 0.44901275634765625\n",
      "Loss: 0.44899311661720276\n",
      "Loss: 0.4489297866821289\n",
      "Loss: 0.4489705562591553\n",
      "Loss: 0.44890156388282776\n",
      "Loss: 0.44886094331741333\n",
      "Loss: 0.4488525986671448\n",
      "Loss: 0.44878265261650085\n",
      "Loss: 0.4486857056617737\n",
      "Loss: 0.4487663209438324\n",
      "Loss: 0.4486598074436188\n",
      "Loss: 0.44858500361442566\n",
      "Loss: 0.4485962986946106\n",
      "Loss: 0.4485590159893036\n",
      "Loss: 0.4484945833683014\n",
      "Loss: 0.4484138488769531\n",
      "Loss: 0.4483306407928467\n",
      "Loss: 0.4482637941837311\n",
      "Loss: 0.44814538955688477\n",
      "Loss: 0.448085218667984\n",
      "Loss: 0.44805121421813965\n",
      "Loss: 0.4479597508907318\n",
      "Loss: 0.4478304982185364\n",
      "Loss: 0.4477613568305969\n",
      "Loss: 0.44770047068595886\n",
      "Loss: 0.4476061463356018\n",
      "Loss: 0.4474835991859436\n",
      "Loss: 0.44732895493507385\n",
      "Loss: 0.4471377432346344\n",
      "Loss: 0.44692903757095337\n",
      "Loss: 0.4467228949069977\n",
      "Loss: 0.446484237909317\n",
      "Loss: 0.4464775025844574\n",
      "Loss: 0.44634199142456055\n",
      "Loss: 0.4459974467754364\n",
      "Loss: 0.44620952010154724\n",
      "Loss: 0.4457162022590637\n",
      "Loss: 0.44583994150161743\n",
      "Loss: 0.44501373171806335\n",
      "Loss: 0.4449525773525238\n",
      "Loss: 0.44451257586479187\n",
      "Loss: 0.4446885585784912\n",
      "Loss: 0.4444875419139862\n",
      "Loss: 0.4440441131591797\n",
      "Loss: 0.44351324439048767\n",
      "Loss: 0.44379639625549316\n",
      "Loss: 0.4438090920448303\n",
      "Loss: 0.4484662115573883\n",
      "Loss: 0.4421338737010956\n",
      "Loss: 0.44856759905815125\n",
      "Loss: 0.4492188096046448\n",
      "Loss: 0.44940873980522156\n",
      "Loss: 0.4487379491329193\n",
      "Loss: 0.44917699694633484\n",
      "Loss: 0.44810962677001953\n",
      "Loss: 0.44792452454566956\n",
      "Loss: 0.44751834869384766\n",
      "Loss: 0.44743308424949646\n",
      "Loss: 0.44769617915153503\n",
      "Loss: 0.44718050956726074\n",
      "Loss: 0.4471644461154938\n",
      "Loss: 0.447314590215683\n",
      "Loss: 0.44716721773147583\n",
      "Loss: 0.4471034109592438\n",
      "Loss: 0.44675111770629883\n",
      "Loss: 0.4470195770263672\n",
      "Loss: 0.4467040002346039\n",
      "Loss: 0.4465753138065338\n",
      "Loss: 0.4464382231235504\n",
      "Loss: 0.4464380443096161\n",
      "Loss: 0.4461231827735901\n",
      "Loss: 0.4460485875606537\n",
      "Loss: 0.44590267539024353\n",
      "Loss: 0.44562968611717224\n",
      "Loss: 0.44554463028907776\n",
      "Loss: 0.4452613890171051\n",
      "Loss: 0.44499656558036804\n",
      "Loss: 0.44422054290771484\n",
      "Loss: 0.4402695596218109\n",
      "Loss: 0.43973374366760254\n",
      "Loss: 0.4409582316875458\n",
      "Loss: 0.4393117427825928\n",
      "Loss: 0.4436560273170471\n",
      "Loss: 0.44398805499076843\n",
      "Loss: 0.438425749540329\n",
      "Loss: 0.4372793734073639\n",
      "Loss: 0.44227296113967896\n",
      "Loss: 0.4385836720466614\n",
      "Loss: 0.4387916624546051\n",
      "Loss: 0.43860021233558655\n",
      "Loss: 0.4373248219490051\n",
      "Loss: 0.43885526061058044\n",
      "Loss: 0.4364650845527649\n",
      "Loss: 0.43713709712028503\n",
      "Loss: 0.4363020062446594\n",
      "Loss: 0.4359603524208069\n",
      "Loss: 0.4361768662929535\n",
      "Loss: 0.43494659662246704\n",
      "Loss: 0.4341756999492645\n",
      "Loss: 0.4339771270751953\n",
      "Loss: 0.43352943658828735\n",
      "Loss: 0.4331377446651459\n",
      "Loss: 0.4321947693824768\n",
      "Loss: 0.43197008967399597\n",
      "Loss: 0.43537431955337524\n",
      "Loss: 0.4364319443702698\n",
      "Loss: 0.43212372064590454\n",
      "Loss: 0.44733235239982605\n",
      "Loss: 0.44820472598075867\n",
      "Loss: 0.4310756325721741\n",
      "Loss: 0.43875908851623535\n",
      "Loss: 0.4350687563419342\n",
      "Loss: 0.43567290902137756\n",
      "Loss: 0.43034791946411133\n",
      "Loss: 0.43491998314857483\n",
      "Loss: 0.42911291122436523\n",
      "Loss: 0.4402712285518646\n",
      "Loss: 0.4353671669960022\n",
      "Loss: 0.4410669803619385\n",
      "Loss: 0.4368824064731598\n",
      "Loss: 0.4420609176158905\n",
      "Loss: 0.449400395154953\n",
      "Loss: 0.44490453600883484\n",
      "Loss: 0.4370032846927643\n",
      "Loss: 0.4339442253112793\n",
      "Loss: 0.4350441098213196\n",
      "Loss: 0.43348726630210876\n",
      "Loss: 0.4305368959903717\n",
      "Loss: 0.43785127997398376\n",
      "Loss: 0.4335269629955292\n",
      "Loss: 0.4430198669433594\n",
      "Loss: 0.44391632080078125\n",
      "Loss: 0.4453616142272949\n",
      "Loss: 0.4377053678035736\n",
      "Loss: 0.4298318028450012\n",
      "Loss: 0.43231964111328125\n",
      "Loss: 0.43752557039260864\n",
      "Loss: 0.4370107650756836\n",
      "Loss: 0.4386472702026367\n",
      "Loss: 0.43636780977249146\n",
      "Loss: 0.43804046511650085\n",
      "Loss: 0.44210195541381836\n",
      "Loss: 0.43404385447502136\n",
      "Loss: 0.4418693482875824\n",
      "Loss: 0.445244699716568\n",
      "Loss: 0.4464204013347626\n",
      "Loss: 0.4375741481781006\n",
      "Loss: 0.42704343795776367\n",
      "Loss: 0.44539976119995117\n",
      "Loss: 0.44560548663139343\n",
      "Loss: 0.4298696517944336\n",
      "Loss: 0.43788883090019226\n",
      "Loss: 0.443364679813385\n",
      "Loss: 0.4463302791118622\n",
      "Loss: 0.44544562697410583\n",
      "Loss: 0.44355425238609314\n",
      "Loss: 0.4414709210395813\n",
      "Loss: 0.4400944709777832\n",
      "Loss: 0.44092264771461487\n",
      "Loss: 0.4421946108341217\n",
      "Loss: 0.4416056275367737\n",
      "Loss: 0.4399746358394623\n",
      "Loss: 0.43902501463890076\n",
      "Loss: 0.4380694031715393\n",
      "Loss: 0.4382766783237457\n",
      "Loss: 0.43913817405700684\n",
      "Loss: 0.43885374069213867\n",
      "Loss: 0.43778467178344727\n",
      "Loss: 0.43795910477638245\n",
      "Loss: 0.43827077746391296\n",
      "Loss: 0.43784481287002563\n",
      "Loss: 0.43784841895103455\n",
      "Loss: 0.4376589357852936\n",
      "Loss: 0.43674325942993164\n",
      "Loss: 0.43599367141723633\n",
      "Loss: 0.4349084496498108\n",
      "Loss: 0.4329220950603485\n",
      "Loss: 0.43585777282714844\n",
      "Loss: 0.42912527918815613\n",
      "Loss: 0.4363071620464325\n",
      "Loss: 0.43918201327323914\n",
      "Loss: 0.43889355659484863\n",
      "Loss: 0.43524765968322754\n",
      "Loss: 0.4295079708099365\n",
      "Loss: 0.44220200181007385\n",
      "Loss: 0.4459288716316223\n",
      "Loss: 0.4289376437664032\n",
      "Loss: 0.43579980731010437\n",
      "Loss: 0.44190025329589844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.44544848799705505\n",
      "Loss: 0.44403859972953796\n",
      "Loss: 0.44055911898612976\n",
      "Loss: 0.43759337067604065\n",
      "Loss: 0.43737897276878357\n",
      "Loss: 0.4388492703437805\n",
      "Loss: 0.43996405601501465\n",
      "Loss: 0.439524382352829\n",
      "Loss: 0.4380801320075989\n",
      "Loss: 0.4365186393260956\n",
      "Loss: 0.4357764720916748\n",
      "Loss: 0.4368588626384735\n",
      "Loss: 0.43747687339782715\n",
      "Loss: 0.4372813403606415\n",
      "Loss: 0.43568140268325806\n",
      "Loss: 0.4355982840061188\n",
      "Loss: 0.4357256591320038\n",
      "Loss: 0.4361606538295746\n",
      "Loss: 0.43610525131225586\n",
      "Loss: 0.43529319763183594\n",
      "Loss: 0.43468141555786133\n",
      "Loss: 0.4342148005962372\n",
      "Loss: 0.43448305130004883\n",
      "Loss: 0.43468475341796875\n",
      "Loss: 0.4344322681427002\n",
      "Loss: 0.43388763070106506\n",
      "Loss: 0.433403879404068\n",
      "Loss: 0.43238577246665955\n",
      "Loss: 0.4315546154975891\n",
      "Loss: 0.4293740391731262\n",
      "Loss: 0.43262630701065063\n",
      "Loss: 0.4293522834777832\n",
      "Loss: 0.42892083525657654\n",
      "Loss: 0.42872077226638794\n",
      "Loss: 0.4277072548866272\n",
      "Loss: 0.42766162753105164\n",
      "Loss: 0.4314729869365692\n",
      "Loss: 0.42631635069847107\n",
      "Loss: 0.42728081345558167\n",
      "Loss: 0.42549243569374084\n",
      "Loss: 0.4267789423465729\n",
      "Loss: 0.4257884919643402\n",
      "Loss: 0.43041181564331055\n",
      "Loss: 0.4276869297027588\n",
      "Loss: 0.42739757895469666\n",
      "Loss: 0.42830991744995117\n",
      "Loss: 0.4256816506385803\n",
      "Loss: 0.42559710144996643\n",
      "Loss: 0.4283258616924286\n",
      "Loss: 0.42629262804985046\n",
      "Loss: 0.4261181056499481\n",
      "Loss: 0.42693111300468445\n",
      "Loss: 0.4263177812099457\n",
      "Loss: 0.425445556640625\n",
      "Loss: 0.4280245304107666\n",
      "Loss: 0.426312655210495\n",
      "Loss: 0.4265553951263428\n",
      "Loss: 0.4259676933288574\n",
      "Loss: 0.425880491733551\n",
      "Loss: 0.42533499002456665\n",
      "Loss: 0.42626506090164185\n",
      "Loss: 0.4266148507595062\n",
      "Loss: 0.42576834559440613\n",
      "Loss: 0.42687129974365234\n",
      "Loss: 0.42560282349586487\n",
      "Loss: 0.4256073534488678\n",
      "Loss: 0.4254051148891449\n",
      "Loss: 0.42586347460746765\n",
      "Loss: 0.4252331852912903\n",
      "Loss: 0.42583855986595154\n",
      "Loss: 0.425821989774704\n",
      "Loss: 0.425229549407959\n",
      "Loss: 0.42581912875175476\n",
      "Loss: 0.42577624320983887\n",
      "Loss: 0.425386905670166\n",
      "Loss: 0.425447940826416\n",
      "Loss: 0.4253826141357422\n",
      "Loss: 0.4250344932079315\n",
      "Loss: 0.4248908460140228\n",
      "Loss: 0.4257994592189789\n",
      "Loss: 0.42500218749046326\n",
      "Loss: 0.4255494177341461\n",
      "Loss: 0.42544203996658325\n",
      "Loss: 0.42517563700675964\n",
      "Loss: 0.424500048160553\n",
      "Loss: 0.4255715012550354\n",
      "Loss: 0.4248717725276947\n",
      "Loss: 0.4247754216194153\n",
      "Loss: 0.4256543219089508\n",
      "Loss: 0.4250221252441406\n",
      "Loss: 0.4244687557220459\n",
      "Loss: 0.4256173074245453\n",
      "Loss: 0.4249531924724579\n",
      "Loss: 0.4243530333042145\n",
      "Loss: 0.4254825711250305\n",
      "Loss: 0.42493611574172974\n",
      "Loss: 0.42381033301353455\n",
      "Loss: 0.4255281984806061\n",
      "Loss: 0.42472514510154724\n",
      "Loss: 0.4238980710506439\n",
      "Loss: 0.42557117342948914\n",
      "Loss: 0.42480728030204773\n",
      "Loss: 0.4236709177494049\n",
      "Loss: 0.4254055917263031\n",
      "Loss: 0.4247381389141083\n",
      "Loss: 0.4232315123081207\n",
      "Loss: 0.4254336357116699\n",
      "Loss: 0.4246235489845276\n",
      "Loss: 0.4230530858039856\n",
      "Loss: 0.42539340257644653\n",
      "Loss: 0.42457881569862366\n",
      "Loss: 0.4227341115474701\n",
      "Loss: 0.42405304312705994\n",
      "Loss: 0.42440661787986755\n",
      "Loss: 0.4220729470252991\n",
      "Loss: 0.4338075816631317\n",
      "Loss: 0.4291023910045624\n",
      "Loss: 0.4279349446296692\n",
      "Loss: 0.4305996596813202\n",
      "Loss: 0.4227808117866516\n",
      "Loss: 0.4272685647010803\n",
      "Loss: 0.4219498634338379\n",
      "Loss: 0.43525075912475586\n",
      "Loss: 0.4251653254032135\n",
      "Loss: 0.43117672204971313\n",
      "Loss: 0.43107327818870544\n",
      "Loss: 0.43122830986976624\n",
      "Loss: 0.4305899739265442\n",
      "Loss: 0.42975932359695435\n",
      "Loss: 0.4352130889892578\n",
      "Loss: 0.4237689673900604\n",
      "Loss: 0.4333224296569824\n",
      "Loss: 0.437009334564209\n",
      "Loss: 0.44146308302879333\n",
      "Loss: 0.4405995309352875\n",
      "Loss: 0.43806192278862\n",
      "Loss: 0.437983900308609\n",
      "Loss: 0.43578240275382996\n",
      "Loss: 0.4368324279785156\n",
      "Loss: 0.43743377923965454\n",
      "Loss: 0.4370301365852356\n",
      "Loss: 0.437296599149704\n",
      "Loss: 0.43598148226737976\n",
      "Loss: 0.435284286737442\n",
      "Loss: 0.4350306987762451\n",
      "Loss: 0.43516770005226135\n",
      "Loss: 0.43496137857437134\n",
      "Loss: 0.43530845642089844\n",
      "Loss: 0.43514424562454224\n",
      "Loss: 0.434518426656723\n",
      "Loss: 0.4344049394130707\n",
      "Loss: 0.43361401557922363\n",
      "Loss: 0.4337310492992401\n",
      "Loss: 0.4336404800415039\n",
      "Loss: 0.43363887071609497\n",
      "Loss: 0.4336114525794983\n",
      "Loss: 0.43230924010276794\n",
      "Loss: 0.4315529763698578\n",
      "Loss: 0.4306733310222626\n",
      "Loss: 0.42934274673461914\n",
      "Loss: 0.4273117184638977\n",
      "Loss: 0.42867952585220337\n",
      "Loss: 0.4318626821041107\n",
      "Loss: 0.4336049258708954\n",
      "Loss: 0.4366453289985657\n",
      "Loss: 0.4354856014251709\n",
      "Loss: 0.43348851799964905\n",
      "Loss: 0.43237096071243286\n",
      "Loss: 0.43132758140563965\n",
      "Loss: 0.43102917075157166\n",
      "Loss: 0.43162956833839417\n",
      "Loss: 0.42946338653564453\n",
      "Loss: 0.4516158103942871\n",
      "Loss: 0.4338587820529938\n",
      "Loss: 0.4352879524230957\n",
      "Loss: 0.4429588317871094\n",
      "Loss: 0.4471568763256073\n",
      "Loss: 0.442864328622818\n",
      "Loss: 0.4390571713447571\n",
      "Loss: 0.43647947907447815\n",
      "Loss: 0.4337298572063446\n",
      "Loss: 0.4373631179332733\n",
      "Loss: 0.4389253258705139\n",
      "Loss: 0.4395983815193176\n",
      "Loss: 0.43875154852867126\n",
      "Loss: 0.4379103183746338\n",
      "Loss: 0.4358128607273102\n",
      "Loss: 0.4343477189540863\n",
      "Loss: 0.4346953332424164\n",
      "Loss: 0.43556639552116394\n",
      "Loss: 0.4364507794380188\n",
      "Loss: 0.43644386529922485\n",
      "Loss: 0.435549259185791\n",
      "Loss: 0.43418699502944946\n",
      "Loss: 0.43370911478996277\n",
      "Loss: 0.433826208114624\n",
      "Loss: 0.4345432221889496\n",
      "Loss: 0.4345208704471588\n",
      "Loss: 0.43373605608940125\n",
      "Loss: 0.4332504868507385\n",
      "Loss: 0.4330717623233795\n",
      "Loss: 0.4331028461456299\n",
      "Loss: 0.43314462900161743\n",
      "Loss: 0.43316397070884705\n",
      "Loss: 0.43297138810157776\n",
      "Loss: 0.4326709806919098\n",
      "Loss: 0.43239933252334595\n",
      "Loss: 0.4323025643825531\n",
      "Loss: 0.4323073923587799\n",
      "Loss: 0.4323280453681946\n",
      "Loss: 0.4321020245552063\n",
      "Loss: 0.43178173899650574\n",
      "Loss: 0.4310261905193329\n",
      "Loss: 0.4307684600353241\n",
      "Loss: 0.43074414134025574\n",
      "Loss: 0.43000683188438416\n",
      "Loss: 0.42995044589042664\n",
      "Loss: 0.4285927414894104\n",
      "Loss: 0.4271090626716614\n",
      "Loss: 0.43241262435913086\n",
      "Loss: 0.43141552805900574\n",
      "Loss: 0.4347773492336273\n",
      "Loss: 0.44073811173439026\n",
      "Loss: 0.4417073726654053\n",
      "Loss: 0.4393623471260071\n",
      "Loss: 0.4365668296813965\n",
      "Loss: 0.4355201721191406\n",
      "Loss: 0.4340244233608246\n",
      "Loss: 0.4346235692501068\n",
      "Loss: 0.43443164229393005\n",
      "Loss: 0.4338940680027008\n",
      "Loss: 0.43344685435295105\n",
      "Loss: 0.4334418475627899\n",
      "Loss: 0.43302980065345764\n",
      "Loss: 0.43366503715515137\n",
      "Loss: 0.43382006883621216\n",
      "Loss: 0.4336977005004883\n",
      "Loss: 0.4337519705295563\n",
      "Loss: 0.4334051311016083\n",
      "Loss: 0.4329589903354645\n",
      "Loss: 0.4326169490814209\n",
      "Loss: 0.4323279857635498\n",
      "Loss: 0.43195438385009766\n",
      "Loss: 0.43193578720092773\n",
      "Loss: 0.4318476617336273\n",
      "Loss: 0.4318080544471741\n",
      "Loss: 0.4317696988582611\n",
      "Loss: 0.4317595660686493\n",
      "Loss: 0.4316409230232239\n",
      "Loss: 0.43170902132987976\n",
      "Loss: 0.431744247674942\n",
      "Loss: 0.4316904544830322\n",
      "Loss: 0.4316217601299286\n",
      "Loss: 0.4315631091594696\n",
      "Loss: 0.43144065141677856\n",
      "Loss: 0.4311642646789551\n",
      "Loss: 0.43057987093925476\n",
      "Loss: 0.4304954707622528\n",
      "Loss: 0.43034037947654724\n",
      "Loss: 0.4300966262817383\n",
      "Loss: 0.430119127035141\n",
      "Loss: 0.4299619197845459\n",
      "Loss: 0.42978373169898987\n",
      "Loss: 0.4295136034488678\n",
      "Loss: 0.4270051419734955\n",
      "Loss: 0.4307214319705963\n",
      "Loss: 0.4316493570804596\n",
      "Loss: 0.43557658791542053\n",
      "Loss: 0.43933987617492676\n",
      "Loss: 0.4395773410797119\n",
      "Loss: 0.43807268142700195\n",
      "Loss: 0.4340638220310211\n",
      "Loss: 0.43366703391075134\n",
      "Loss: 0.4343373775482178\n",
      "Loss: 0.4346866309642792\n",
      "Loss: 0.4352608323097229\n",
      "Loss: 0.4344661831855774\n",
      "Loss: 0.4332399368286133\n",
      "Loss: 0.43245455622673035\n",
      "Loss: 0.4322468936443329\n",
      "Loss: 0.4323202669620514\n",
      "Loss: 0.4328480362892151\n",
      "Loss: 0.4331822395324707\n",
      "Loss: 0.4329175055027008\n",
      "Loss: 0.43239814043045044\n",
      "Loss: 0.43217936158180237\n",
      "Loss: 0.43197932839393616\n",
      "Loss: 0.43199214339256287\n",
      "Loss: 0.4321351945400238\n",
      "Loss: 0.4322715401649475\n",
      "Loss: 0.4321077764034271\n",
      "Loss: 0.4318905770778656\n",
      "Loss: 0.4316905438899994\n",
      "Loss: 0.4315250813961029\n",
      "Loss: 0.43140172958374023\n",
      "Loss: 0.4314349293708801\n",
      "Loss: 0.4314913749694824\n",
      "Loss: 0.43147459626197815\n",
      "Loss: 0.43141433596611023\n",
      "Loss: 0.43133702874183655\n",
      "Loss: 0.43122759461402893\n",
      "Loss: 0.4311716556549072\n",
      "Loss: 0.43119335174560547\n",
      "Loss: 0.4312211573123932\n",
      "Loss: 0.43123307824134827\n",
      "Loss: 0.43123432993888855\n",
      "Loss: 0.4311809539794922\n",
      "Loss: 0.4311281144618988\n",
      "Loss: 0.43110328912734985\n",
      "Loss: 0.43109211325645447\n",
      "Loss: 0.4310935139656067\n",
      "Loss: 0.43110400438308716\n",
      "Loss: 0.43108177185058594\n",
      "Loss: 0.4310533106327057\n",
      "Loss: 0.4310227930545807\n",
      "Loss: 0.43099555373191833\n",
      "Loss: 0.43098244071006775\n",
      "Loss: 0.43098345398902893\n",
      "Loss: 0.4309755265712738\n",
      "Loss: 0.43096333742141724\n",
      "Loss: 0.4309441149234772\n",
      "Loss: 0.4309212863445282\n",
      "Loss: 0.4309022128582001\n",
      "Loss: 0.43089398741722107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4308857023715973\n",
      "Loss: 0.43087682127952576\n",
      "Loss: 0.43086716532707214\n",
      "Loss: 0.4308602511882782\n",
      "Loss: 0.4308500289916992\n",
      "Loss: 0.4308426082134247\n",
      "Loss: 0.43083611130714417\n",
      "Loss: 0.43082910776138306\n",
      "Loss: 0.43082156777381897\n",
      "Loss: 0.43081504106521606\n",
      "Loss: 0.4308067858219147\n",
      "Loss: 0.4307992160320282\n",
      "Loss: 0.4307912290096283\n",
      "Loss: 0.4307463765144348\n",
      "Loss: 0.43044567108154297\n",
      "Loss: 0.4302522838115692\n",
      "Loss: 0.4302145540714264\n",
      "Loss: 0.43021222949028015\n",
      "Loss: 0.4301634728908539\n",
      "Loss: 0.4300857484340668\n",
      "Loss: 0.4299876391887665\n",
      "Loss: 0.42953696846961975\n",
      "Loss: 0.4292953908443451\n",
      "Loss: 0.4292401373386383\n",
      "Loss: 0.4292328357696533\n",
      "Loss: 0.4291853606700897\n",
      "Loss: 0.42911654710769653\n",
      "Loss: 0.4288499057292938\n",
      "Loss: 0.4262668788433075\n",
      "Loss: 0.43281012773513794\n",
      "Loss: 0.43223172426223755\n",
      "Loss: 0.435034841299057\n",
      "Loss: 0.4402187764644623\n",
      "Loss: 0.44090184569358826\n",
      "Loss: 0.43937644362449646\n",
      "Loss: 0.4348908066749573\n",
      "Loss: 0.43414339423179626\n",
      "Loss: 0.43311840295791626\n",
      "Loss: 0.43350887298583984\n",
      "Loss: 0.4354492127895355\n",
      "Loss: 0.4352082908153534\n",
      "Loss: 0.43509531021118164\n",
      "Loss: 0.4340085983276367\n",
      "Loss: 0.43377548456192017\n",
      "Loss: 0.4331434369087219\n",
      "Loss: 0.4335181415081024\n",
      "Loss: 0.43364962935447693\n",
      "Loss: 0.4332147240638733\n",
      "Loss: 0.4332675039768219\n",
      "Loss: 0.43281587958335876\n",
      "Loss: 0.43268510699272156\n",
      "Loss: 0.43221697211265564\n",
      "Loss: 0.43213769793510437\n",
      "Loss: 0.4322132170200348\n",
      "Loss: 0.4321298599243164\n",
      "Loss: 0.4323123097419739\n",
      "Loss: 0.43213969469070435\n",
      "Loss: 0.4319295883178711\n",
      "Loss: 0.43159720301628113\n",
      "Loss: 0.4314112365245819\n",
      "Loss: 0.4314699172973633\n",
      "Loss: 0.43130573630332947\n",
      "Loss: 0.4313342571258545\n",
      "Loss: 0.43125078082084656\n",
      "Loss: 0.4311632215976715\n",
      "Loss: 0.4310474395751953\n",
      "Loss: 0.43092823028564453\n",
      "Loss: 0.4308912456035614\n",
      "Loss: 0.43030738830566406\n",
      "Loss: 0.4303707778453827\n",
      "Loss: 0.43039342761039734\n",
      "Loss: 0.4303850829601288\n",
      "Loss: 0.42993101477622986\n",
      "Loss: 0.4295313060283661\n",
      "Loss: 0.4293517768383026\n",
      "Loss: 0.4292432963848114\n",
      "Loss: 0.4292139708995819\n",
      "Loss: 0.4291907250881195\n",
      "Loss: 0.4292363226413727\n",
      "Loss: 0.42917394638061523\n",
      "Loss: 0.42920494079589844\n",
      "Loss: 0.4290061593055725\n",
      "Loss: 0.42681318521499634\n",
      "Loss: 0.4489103853702545\n",
      "Loss: 0.4334718883037567\n",
      "Loss: 0.4390893578529358\n",
      "Loss: 0.444016695022583\n",
      "Loss: 0.44465571641921997\n",
      "Loss: 0.44391727447509766\n",
      "Loss: 0.4389088749885559\n",
      "Loss: 0.4353877604007721\n",
      "Loss: 0.43693748116493225\n",
      "Loss: 0.43523940443992615\n",
      "Loss: 0.43682727217674255\n",
      "Loss: 0.4381117820739746\n",
      "Loss: 0.43622827529907227\n",
      "Loss: 0.43609681725502014\n",
      "Loss: 0.4346526265144348\n",
      "Loss: 0.43448325991630554\n",
      "Loss: 0.43448376655578613\n",
      "Loss: 0.43452006578445435\n",
      "Loss: 0.4357818067073822\n",
      "Loss: 0.4353233277797699\n",
      "Loss: 0.4347855746746063\n",
      "Loss: 0.433960497379303\n",
      "Loss: 0.4333251416683197\n",
      "Loss: 0.43317294120788574\n",
      "Loss: 0.4330931305885315\n",
      "Loss: 0.4332641065120697\n",
      "Loss: 0.43326225876808167\n",
      "Loss: 0.4329527020454407\n",
      "Loss: 0.4328523278236389\n",
      "Loss: 0.43258604407310486\n",
      "Loss: 0.43243226408958435\n",
      "Loss: 0.43244943022727966\n",
      "Loss: 0.43244919180870056\n",
      "Loss: 0.4324363172054291\n",
      "Loss: 0.4323189854621887\n",
      "Loss: 0.43224191665649414\n",
      "Loss: 0.43212124705314636\n",
      "Loss: 0.4318758547306061\n",
      "Loss: 0.4317193627357483\n",
      "Loss: 0.43158942461013794\n",
      "Loss: 0.4315025508403778\n",
      "Loss: 0.43144121766090393\n",
      "Loss: 0.4313977360725403\n",
      "Loss: 0.43135061860084534\n",
      "Loss: 0.4312179684638977\n",
      "Loss: 0.43112045526504517\n",
      "Loss: 0.4310511648654938\n",
      "Loss: 0.43099212646484375\n",
      "Loss: 0.4309506416320801\n",
      "Loss: 0.43097057938575745\n",
      "Loss: 0.4309554100036621\n",
      "Loss: 0.4308817386627197\n",
      "Loss: 0.43084651231765747\n",
      "Loss: 0.4308205246925354\n",
      "Loss: 0.43078622221946716\n",
      "Loss: 0.43075570464134216\n",
      "Loss: 0.43078580498695374\n",
      "Loss: 0.43079814314842224\n",
      "Loss: 0.43076545000076294\n",
      "Loss: 0.43071627616882324\n",
      "Loss: 0.43070903420448303\n",
      "Loss: 0.4306812286376953\n",
      "Loss: 0.4306267201900482\n",
      "Loss: 0.43052369356155396\n",
      "Loss: 0.43062010407447815\n",
      "Loss: 0.4307004511356354\n",
      "Loss: 0.43067073822021484\n",
      "Loss: 0.43050262331962585\n",
      "Loss: 0.43048912286758423\n",
      "Loss: 0.4305875897407532\n",
      "Loss: 0.4306057393550873\n",
      "Loss: 0.43054085969924927\n",
      "Loss: 0.43038806319236755\n",
      "Loss: 0.4304695129394531\n",
      "Loss: 0.4304969310760498\n",
      "Loss: 0.4304828345775604\n",
      "Loss: 0.4302888512611389\n",
      "Loss: 0.4304523169994354\n",
      "Loss: 0.43042635917663574\n",
      "Loss: 0.4305478036403656\n",
      "Loss: 0.4303950369358063\n",
      "Loss: 0.43041905760765076\n",
      "Loss: 0.4302697777748108\n",
      "Loss: 0.4304482936859131\n",
      "Loss: 0.43031126260757446\n",
      "Loss: 0.4304339587688446\n",
      "Loss: 0.43021494150161743\n",
      "Loss: 0.43045663833618164\n",
      "Loss: 0.43037205934524536\n",
      "Loss: 0.430474191904068\n",
      "Loss: 0.43034178018569946\n",
      "Loss: 0.43029385805130005\n",
      "Loss: 0.4302116334438324\n",
      "Loss: 0.43027740716934204\n",
      "Loss: 0.4302414059638977\n",
      "Loss: 0.43016862869262695\n",
      "Loss: 0.4301302134990692\n",
      "Loss: 0.4301491677761078\n",
      "Loss: 0.430094450712204\n",
      "Loss: 0.4302181303501129\n",
      "Loss: 0.430103600025177\n",
      "Loss: 0.4300737977027893\n",
      "Loss: 0.4301608204841614\n",
      "Loss: 0.43018147349357605\n",
      "Loss: 0.43014732003211975\n",
      "Loss: 0.43008795380592346\n",
      "Loss: 0.43010756373405457\n",
      "Loss: 0.4301447868347168\n",
      "Loss: 0.43010401725769043\n",
      "Loss: 0.4302941858768463\n",
      "Loss: 0.4300456941127777\n",
      "Loss: 0.4300622045993805\n",
      "Loss: 0.42996320128440857\n",
      "Loss: 0.43020230531692505\n",
      "Loss: 0.430029958486557\n",
      "Loss: 0.4301985800266266\n",
      "Loss: 0.42994388937950134\n",
      "Loss: 0.43004241585731506\n",
      "Loss: 0.4299636483192444\n",
      "Loss: 0.4299681782722473\n",
      "Loss: 0.43000054359436035\n",
      "Loss: 0.4299662113189697\n",
      "Loss: 0.4299148619174957\n",
      "Loss: 0.42965051531791687\n",
      "Loss: 0.4298055171966553\n",
      "Loss: 0.4296848475933075\n",
      "Loss: 0.4296826422214508\n",
      "Loss: 0.42960405349731445\n",
      "Loss: 0.4295685887336731\n",
      "Loss: 0.4295840263366699\n",
      "Loss: 0.42939844727516174\n",
      "Loss: 0.4295573830604553\n",
      "Loss: 0.42925140261650085\n",
      "Loss: 0.42938539385795593\n",
      "Loss: 0.42921921610832214\n",
      "Loss: 0.42940232157707214\n",
      "Loss: 0.42915159463882446\n",
      "Loss: 0.42957860231399536\n",
      "Loss: 0.4291245937347412\n",
      "Loss: 0.4295904338359833\n",
      "Loss: 0.42913857102394104\n",
      "Loss: 0.4291628897190094\n",
      "Loss: 0.42866334319114685\n",
      "Loss: 0.429369181394577\n",
      "Loss: 0.42870283126831055\n",
      "Loss: 0.42892247438430786\n",
      "Loss: 0.4284941852092743\n",
      "Loss: 0.4286598265171051\n",
      "Loss: 0.4283563196659088\n",
      "Loss: 0.4284837245941162\n",
      "Loss: 0.4283985197544098\n",
      "Loss: 0.4283529818058014\n",
      "Loss: 0.42823049426078796\n",
      "Loss: 0.42801758646965027\n",
      "Loss: 0.42792877554893494\n",
      "Loss: 0.4279828667640686\n",
      "Loss: 0.42796269059181213\n",
      "Loss: 0.42782852053642273\n",
      "Loss: 0.42788171768188477\n",
      "Loss: 0.4279286563396454\n",
      "Loss: 0.42796969413757324\n",
      "Loss: 0.4277960956096649\n",
      "Loss: 0.42769813537597656\n",
      "Loss: 0.4278857707977295\n",
      "Loss: 0.4278021454811096\n",
      "Loss: 0.4277799725532532\n",
      "Loss: 0.4277559816837311\n",
      "Loss: 0.42752858996391296\n",
      "Loss: 0.4275282025337219\n",
      "Loss: 0.4275638163089752\n",
      "Loss: 0.42725619673728943\n",
      "Loss: 0.4255227744579315\n",
      "Loss: 0.4420461356639862\n",
      "Loss: 0.4326595366001129\n",
      "Loss: 0.4355832040309906\n",
      "Loss: 0.44002652168273926\n",
      "Loss: 0.44232073426246643\n",
      "Loss: 0.43926188349723816\n",
      "Loss: 0.43855246901512146\n",
      "Loss: 0.43405023217201233\n",
      "Loss: 0.4354911744594574\n",
      "Loss: 0.4356409013271332\n",
      "Loss: 0.4347756505012512\n",
      "Loss: 0.43692395091056824\n",
      "Loss: 0.43630725145339966\n",
      "Loss: 0.43380483984947205\n",
      "Loss: 0.4341321885585785\n",
      "Loss: 0.43346744775772095\n",
      "Loss: 0.43245741724967957\n",
      "Loss: 0.43346014618873596\n",
      "Loss: 0.43380412459373474\n",
      "Loss: 0.4331945478916168\n",
      "Loss: 0.43325814604759216\n",
      "Loss: 0.4320478141307831\n",
      "Loss: 0.43268346786499023\n",
      "Loss: 0.4317176938056946\n",
      "Loss: 0.4317290484905243\n",
      "Loss: 0.43188339471817017\n",
      "Loss: 0.4313068389892578\n",
      "Loss: 0.43145376443862915\n",
      "Loss: 0.43106624484062195\n",
      "Loss: 0.4310672879219055\n",
      "Loss: 0.43045854568481445\n",
      "Loss: 0.430548757314682\n",
      "Loss: 0.43006953597068787\n",
      "Loss: 0.430124968290329\n",
      "Loss: 0.43017226457595825\n",
      "Loss: 0.4300598204135895\n",
      "Loss: 0.4299549162387848\n",
      "Loss: 0.4295661151409149\n",
      "Loss: 0.42952805757522583\n",
      "Loss: 0.4295984208583832\n",
      "Loss: 0.4295181930065155\n",
      "Loss: 0.4292125403881073\n",
      "Loss: 0.42905402183532715\n",
      "Loss: 0.42903247475624084\n",
      "Loss: 0.429103821516037\n",
      "Loss: 0.4290767312049866\n",
      "Loss: 0.4287983477115631\n",
      "Loss: 0.4291086196899414\n",
      "Loss: 0.42878714203834534\n",
      "Loss: 0.4292992651462555\n",
      "Loss: 0.42904138565063477\n",
      "Loss: 0.42906761169433594\n",
      "Loss: 0.4291260242462158\n",
      "Loss: 0.4286493957042694\n",
      "Loss: 0.4287247657775879\n",
      "Loss: 0.4287126660346985\n",
      "Loss: 0.4287246763706207\n",
      "Loss: 0.4284277856349945\n",
      "Loss: 0.42830386757850647\n",
      "Loss: 0.4282810688018799\n",
      "Loss: 0.42852869629859924\n",
      "Loss: 0.42824050784111023\n",
      "Loss: 0.4281788766384125\n",
      "Loss: 0.4276794493198395\n",
      "Loss: 0.4278431832790375\n",
      "Loss: 0.42758890986442566\n",
      "Loss: 0.4277295768260956\n",
      "Loss: 0.4275369644165039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4279020428657532\n",
      "Loss: 0.4272925555706024\n",
      "Loss: 0.42729830741882324\n",
      "Loss: 0.4272356927394867\n",
      "Loss: 0.42707759141921997\n",
      "Loss: 0.42702850699424744\n",
      "Loss: 0.42700520157814026\n",
      "Loss: 0.4270939528942108\n",
      "Loss: 0.42723575234413147\n",
      "Loss: 0.4271114766597748\n",
      "Loss: 0.427308052778244\n",
      "Loss: 0.42694613337516785\n",
      "Loss: 0.42721039056777954\n",
      "Loss: 0.4271722435951233\n",
      "Loss: 0.4271012842655182\n",
      "Loss: 0.42742329835891724\n",
      "Loss: 0.4273340702056885\n",
      "Loss: 0.42786332964897156\n",
      "Loss: 0.42699310183525085\n",
      "Loss: 0.4273781478404999\n",
      "Loss: 0.4274516999721527\n",
      "Loss: 0.42731761932373047\n",
      "Loss: 0.42686089873313904\n",
      "Loss: 0.4427924156188965\n",
      "Loss: 0.4347112774848938\n",
      "Loss: 0.4398256838321686\n",
      "Loss: 0.44024354219436646\n",
      "Loss: 0.4473641812801361\n",
      "Loss: 0.4431036114692688\n",
      "Loss: 0.43939897418022156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fef08610b70>]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq8AAAHxCAYAAABDDVWHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABcqklEQVR4nO3dd3hb5d3/8c+xnTiTyM4gIVsi7DDksFsKxQEKiGmzS9cvcmn7lKcDmy6gk9qF7oGVpwMaoMUGCoJCsZglQCAWeyVYCSQhy7GVHceO9fvDkSLZmrbkoxO/X9flK/aZX+lYzke37nPfRigUCgkAAACwgAKzCwAAAADSRXgFAACAZRBeAQAAYBmEVwAAAFgG4RUAAACWQXgFAACAZaQdXj0ej0pKSjI+QV1dnRwOhwzDkMPhUF1dXcbHAAAAACTJSHec17KyMgUCAbW3t6d98MrKSjU2Nspms6m8vFw+n0/BYFBut1v19fX9LhoAAABDU9LwGgwGtXTpUtXW1srn88lms6UdXv1+v8rKyuR0OtXc3BxZ7nA4FAgE1NLSIrvdPvBHAAAAgCEjabeBkpISzZ8/Xz6fL+MDh1tWFy5cGHc5La8AAADIVNKW18bGxsj3CxYskKS0W14dDofa2tribm8YRp8WWQAAACCVtPu8JgujcQ+cJKBmeiwAAABAyvFQWaWlpXGX22w2BYPBXJ4aAAAA+6GiXBw0HExtNlvc9eFQGwwGE25jGEYOKgMAAEA2pfkhftbkpOU1HEgTta62tbXFbJdIKBTK6teCBQvy+nhWOWZZWVne1ziUr3e2r49VHrcVrrcVXjtWuDa5OKYVrs1Qvd5cm/w+phly2m0gHFJ7S9biCgAAACSSs/Bqt9sVCATirgsEAqaM8epyufL6eFY6ZrZZ4XFbocZcsMrjtsL1zgUrPG6rHDPbrPC4rVBjLljhcVuhRrPkbLSBqqoqeTweNTc3y+l0Rpb7fD7Nnz9f1dXVqq2tTVyYYZjWHI3k5s2bp6VLl5pdBhLg+uQvrk3+4trkL65NfjMjr2Wt5bV3K2tVVZUkqaamJmZ5OLCG18N63G632SUgCa5P/uLa5C+uTf7i2qC3rLS81tXVqaamRrW1taquro4sr6ysVGNjo5xOp+bNmyefz6dAICC3251yhi1aXgEAAPKbZVtenU6nbDZbTPcASWpoaFBtba2CwaA8Ho9sNptqa2uZGhYAAAD9knbL62Cj5RUAACC/WbblFQAAABgMhFcAAABYBuEVAAAAlkF4BQAAgGUUmV1AMuGx3Vwu134zKwQAAICVeb1eeb1e087PaAMAAADoF0YbAAAAAJIgvAIAAMAyCK8AAACwDMIrAAAALIPwCgAAAMsgvAIAAMAyCK8AAACwDMIrAAAALIPwCgAAAMsgvAIAAMAyCK8AAACwjCKzC0jG7XZLklwul1wul8nVAAAAwOv1yuv1mnZ+IxQKhUw7exKGYShPSwMAAIDMyWt0GwCybMfHa9XRHjS7DAAA9ku0vAJZtsiYoILiYl21a43ZpQAAkFO0vAL7ie6ODrNLAABgv0R4BQAAgGUQXgEAAGAZhFcAAABYBuEVAAAAlkF4BQAAgGUQXgEAAGAZhFcAAABYBuEVAAAAllFkdgHJuN1uSZLL5ZLL5TK5GgAAAHi9Xnm9XtPOz/SwQJYtMiZIkq4JtZpcCQAAucX0sAAAAEAShFcAAABYBuEVAAAAlkF4BQAAgGUQXgEAAGAZhFcAAABYBuEVAAAAlkF4BQAAgGUQXgEAAGAZhFcAAABYBuEVAAAAllFkdgHJuN1uSZLL5ZLL5TK5GgAAAHi9Xnm9XtPOb4RCoZBpZ0/CMAzlaWlAUouMCZKka0KtJlcCAEBumZHX6DYAAAAAyyC8AgAAwDIIrwAAALAMwisAAAAsI+3wWldXJ4fDIcMw5HA4VFdXl9GJqqqqIvuXlZVlvD8AAACQVnitrKxUTU2N2traVFFRoba2NtXU1KiqqirlvsFgUA6HQx6PRzabTRUVFQoGg6qpqdH8+fMH/AAAAAAwdKQcKsvv96usrExOp1PNzc2R5Q6HQ4FAQC0tLbLb7Qn3r6ysVGNjo+rr6yPjtkYvb25ultPp7FsYQ2XBohgqCwAwVOTlUFn19fWSpIULF8ZdHv43kcbGRjmdzpjgGn28W2+9Nf1qAQAAMKSlDK8+n082m61P62h5eXlkfSKBQECSNG/evD7rbDab7HZ70v0BAACAaCnDayAQSNgtwG63RwJqMm1tbQmXB4PBlPsDAAAAUpo3bJWWlsZdbrPZkobPcOiN17rq9/sj+xJgAQAAkI6k4TUcKm02W9z14VCbLHxWV1crGAxq/vz5kVZan8+nM888M2Vx8+bNi/nyeDwp9wEAAED2eTyePtnMDClHGzAMQ+Xl5WpqauqzrqysTH6/P+VdZuGRBaJVVFQoEAgk3J/RBmBVjDYAABgqzMhrRelslKjPajAYTNgqG62hoUE+n09+v1+bNm3S/PnzVV5eLofDkdb+AAAAgJRGeE12U1YgEIg7Rms85eXlkREKovfvvQwAAABIJOUNW+Xl5QoGg/L7/THLwzdhpQqfVVVVcWfSCncjSGeWLgAAAEBKI7yGw2VNTU3M8tra2pj1YfFaaX0+X8zNVuHpYaWevq8AAABAOlKGV6fTqYqKCvl8PpWVlamqqkoOh0M+n09utztmDNi6ujo5HA7V1dVFltXW1spms0VaYCsrK1VSUqJAIKCGhobcPCoAAADsl9Ia57WhoUG1tbUKBoPyeDyy2Wyqra3tMzWs0+nsMxuXzWZTc3OzKioqtHTp0sh0sU1NTbS6AgAAICMph8oyC0NlwaoYKgsAMFSYkdfSankFAAAA8gHhFQAAAJZBeAUAAIBlEF4BAABgGWlND2sWt9stSXK5XHK5XCZXAwAAAK/XK6/Xa9r5GW0AyDJGGwAADBWMNgAAAAAkQXgFAACAZRBeAQAAYBmEVwAAAFgG4RUAAACWQXgFAACAZRBeAQAAYBmEVwAAAFgG4RUAAACWQXgFAACAZRBeAQAAYBlFZheQjNvtliS5XC65XC6TqwEAAIDX65XX6zXt/EYoFAqZdvYkDMNQnpYGJLXImCBJuibUanIlAADklhl5jW4DAAAAsAzCKwAAACyD8AoAAADLILwCAADAMgivAAAAsAzCKwAAACyD8AoAAADLILwCAADAMgivAAAAsAzCKwAAACyD8AoAAADLKDK7gGTcbrckyeVyyeVymVwNAAAAvF6vvF6vaec3QqFQyLSzJ2EYhvK0NCCpRcYESdI1oVaTKwEAILfMyGt0GwAAAIBlEF4BAABgGYRXAAAAWAbhFQAAAJZBeAUAAIBlEF4BAABgGYRXAAAAWAbhFQAAAJZBeAUAAIBlEF4BAABgGYRXAAAAWEaR2QUk43a7JUkul0sul8vkagAAAOD1euX1ek07vxEKhUKmnT0JwzCUp6UBSS0yJkiSrgm1mlwJAAC5ZUZeo9sAAAAALIPwCgAAAMsgvAIAAMAyCK8AAACwjLTDa11dnRwOhwzDkMPhUF1dXUYnqqmpGdD+AAAAQFrhtbKyUjU1NWpra1NFRYXa2tpUU1OjqqqqtE5SVlamuro62Ww2VVRUSOoJs2VlZf2vHAAAAENOyvDq9/vV2Ngop9Op9vZ2NTQ0qL29XXa7XR6PR4FAIOn+Ho9Hfr9fbrdbzc3NamhoUEtLiyoqKuT3++XxeLL2YAAAALB/Sxle6+vrJUkLFy6Muzz8byJNTU2Selpao9XW1kqSmpub0ywVAAAAQ13K8Orz+WSz2eR0OmOWl5eXR9YnEwwGJUmlpaVx17e1taVTJwAAAJA6vAYCAdnt9rjr7HZ7ym4D8+fPlyTdeuutMcvDLbbh9QAAAEAqRelslKjV1GazpQyv1dXVamlpUV1dnfx+v5xOp3w+n/x+v6qrq+V2uzOvGgAAAENS0vAa/sjfZrPFXR8OtcFgMOE2kiKjCvh8vphuBscff3zS4ubNmxfzs9vtJuwCAACYwOPx5MWN9kYoFAol3cAwVF5eHrnxKlpZWZn8fr+SHaKmpkZ1dXWqqKhQbW2t7Ha7/H6/ampq5PP5VFtbq+rq6rjnTVEakJcWGRMkSdeEWk2uBACA3DIjr6UVXp1OZ9xRARwOh9ra2tTe3h5332AwqJKSEtntdrW0tMTdPxAIxH3QhFdYFeEVADBUmJHXUt6wleymrGQ3c4XXS/tGJugtPIJBqn6zAAAAgJRGeC0vL1cwGJTf749ZHu67miiYSooE20ThNNynNlkABgAAAMJShtfwFLCJJhnoPUVsdFC12Wyy2+19btSSpMbGRvl8vj7jxwIAAACJpAyvTqdTFRUV8vl8KisrU1VVlRwOh3w+n9xud0yraV1dnRwOh+rq6iLLGhoaJPWM51pWVqbKysrIv9HrAQAAgFRShlepJ2DW1tYqGAzK4/HIZrOptra2z9SwTqezz2xcTqdT7e3tcrvdCgaDamxsVDAYlNvtVnt7O10GAAAAkLaUow2YhdEGYFWMNgAAGCrycrQBAAAAIF8QXgEAAGAZhFcAAABYBuEVAAAAllFkdgHJuN1uSZLL5ZLL5TK5GgAAAHi9Xnm9XtPOz2gDQJYx2gAAYKhgtAEAAAAgCcIrAAAALIPwCgAAAMsgvAIAAMAyCK8AAACwDMIrAAAALIPwCgAAAMsgvAIAAMAyCK8AAACwDMIrAAAALIPwCgAAAMsoMruAZNxutyTJ5XLJ5XKZXA0AAAC8Xq+8Xq9p5zdCoVDItLMnYRiG8rQ0IKlFxgRJ0jWhVpMrAQAgt8zIa3QbAAAAgGUQXgEAAGAZhFcAAABYBuEVAAAAlkF4BQAAgGUQXgEAAGAZhFcAAABYBuEVAAAAlkF4BQAAgGUQXgEAAGAZRWYXkIzb7ZYkuVwuuVwuk6sBAACA1+uV1+s17fxGaLAnpE2TGXPlAtmwyJggSbom1GpyJQAA5JYZeY1uAwAAALAMwisAAAAsg/AKAAAAyyC8AgAAwDIIrwAAALAMwisAAAAsg/AKAAAAyyC8AgAAwDIIrwAAALAMwisAAAAsg/AKAAAAyygyu4Bk3G63JMnlcsnlcplcDQAAALxer7xer2nnN0KhUMi0sydhGIbytDQgqUXGBEnSNaFWkysBACC3zMhrdBsAAACAZRBeAQAAYBmEVwAAAFgG4RUAAACWkXZ4raurk8PhkGEYcjgcqqury2VdAAAAQB9phdfKykrV1NSora1NFRUVamtrU01NjaqqqpLuFwwGZRhGyq/GxsasPBgAAADs31KO8+r3+9XY2Cin06nm5ubIcofDIY/Ho5qaGtnt9rj72mw2OZ3OhMcOBAIKBoOy2WyZVw4AAIAhJ2V4ra+vlyQtXLiwz/L58+ervr5etbW1CfePDrzRgsGgZs+erYqKCpWXl2dSMwAAAIaolJMUOBwOtbW1qb29ve/OhtGnRTZdlZWV8vv9amlpiV8YkxTAopikAAAwVJiR11K2vAYCgYQf/dvtdgUCgYxP2tjYqMbGxn6FXgAAAAxdad2wVVpaGne5zWZTMBjM+KQLFixQRUVF0v6wAAAAQG9JW17DwTTRDVXhUJvJTVd1dXUKBoNJ+8mGzZs3L+Znt9stt9ud1nkAAACQPR6PRx6Px+wyUvd5NQxD5eXlampq6rOurKxMfr8/o74OJSUluuyyyyI3giU7L31eYUX0eQUADBVm5LW0ug20tbXFXZ7pMFcej0fBYDDl+LAAAABAPIM62oDD4ZCkhCMM9D42La+wIlpeAQBDRV62vJaXlysYDMrv98cs9/l8kfXp8Pv9CgQCtLoCAACg31KG13DYrKmpiVkevuGqdxhNNHTWP//5T0nph10AAACgt5Th1el0qqKiQj6fT2VlZaqqqpLD4ZDP55Pb7Y6ZGraurk4Oh0N1dXV9jtPY2Bg5HgAAANAfad2w1dDQoNraWgWDQXk8HtlsNtXW1vYZMcDpdMpms/UJqMFgUIFAgFZXAAAADEjKG7bMwg1bsCpu2AIADBV5ecMWAAAAkC8IrwAAALAMwisAAAAsg/AKAAAAyygyu4Bk3G63JMnlcsnlcplcDQAAALxer7xer2nnZ7QBIMsYbQAAMFQw2gAAAACQBOEVAAAAlkF4BQAAgGUQXgEAAGAZhFcAAABYBuEVyBFGywAAIPsIrwAAALAMwiuQK7S8AgCQdYRXAAAAWAbhFcgR+rwCAJB9hFcAAABYRpHZBSTjdrslSS6XSy6Xy+RqAAAA4PV65fV6TTu/EcrTzzYNw+BjV1jSImOCJOmqznUqKMrr94cAAAyIGXmNbgMAAACwDMIrkCt8cgAAQNYRXgEAAGAZhFcgR+izDQBA9hFeAQAAYBmEVyBXaHkFACDrCK8AAACwDMIrkCu0vAIAkHWEVwAAAFgG4RXIEUYbAAAg+wivAAAAsIy8nnjd7XZLklwul1wul8nVABmi5RUAsB/yer3yer2mnd8I5elnm4Zh8LErLGmRMUGSdMX2j1Q0apTJ1QAAkDtm5DW6DQAAAMAyCK9ArvDJAQAAWUd4BQAAgGUQXoEcoc82AADZR3gFAACAZRBegVyh5RUAgKwjvAIAAMAyCK9AjtDwCgBA9hFeAQAAYBmEVyBXaHoFACDrCK8AAACwjCKzC0jG7XZLklwul1wul8nVABmi5RUAsB/yer3yer2mnd8I5elI6oZhMMg7LGmRMUGSdFl7i4bbxplcDQAAuWNGXqPbAAAAACyD8ArkCJ8cAACQfYRXAAAAWAbhFcgVWl4BAMi6tMNrXV2dHA6HDMOQw+FQXV1dRify+XwqKyuTYRgqKSlRZWWlgsFgpvUCQ1p3Z6f2dHSYXQYAAKZJK7xWVlaqpqZGbW1tqqioUFtbm2pqalRVVZXWSTwej+bPn69AIKCKigrNmzdPjY2Nmj17NgEW+68ctLw+fOhJunfE1KwfFwAAq0gZXv1+vxobG+V0OtXe3q6Ghga1t7fLbrfL4/EoEAikPElVVZXsdrtWrFihhoYGNTU1qb6+XsFgULfeemtWHggwFGxb8aHZJQAAYKqU4bW+vl6StHDhwrjLw/8m4vF4ItvZbLbIcrfbrfLyclpesd8KdXebXQIAAPudlJMUOBwOtbW1qb29ve/OhiGn06nm5uaE+5eVlSkQCMTdP2lhTFIAiwpPUnDJx29p1JTJOTn2NaHWrB4XAID+MCOvpZweNhAIyOl0xl1nt9tTdhsIBAKy2+2Sem7aampq0vjx41VeXp7wuMD+INTZZXYJAADsd1KGV0kqLS2Nu9xms6UMr8FgUKWlpZo/f758Pl/MuoqKCjU0NKRZKmAt3bt3m10CAAD7naR9XsP9UaP7qkYLh9pE/VbDy30+nwKBgJqamhQKhdTS0qLy8nI1NjYmHXJr3rx5MV/h/rOAFXTT8goA2I94PJ4+2cwMKfu8Goah8vJyNTU19VlXVlYmv9+fsK9DMBhUSUmJJKmlpSXSfSCspKREwWAw7v70eYVVhfulnvf6syo5+sicHJs+rwCAfGBGXktrnNe2tra4y4PBYMJWWWlfi63dbu8TXCWpvLw8chxgf9Pd2Wl2CQAA7HdShtdkN2VF34yVSLJwG5YoHANWRrcBAACyL2V4DY/F6vf7Y5aHb74Kt54m2z8QCMRtXQ0fM1UABtLV3dWlFfc05kWXE27YAgAg+1KG1/AUsDU1NTHLa2trY9aH9W6lDa9fsGBBzPK6ujoFAgG53e4MSwYSe+e2P2jx1V/WirvNH8WCllcAALIvZXh1Op2qqKiQz+dTWVmZqqqq5HA45PP55Ha7Y1pN6+rq5HA4YkYQKC8vj4ws4HA4VFlZqbKyMtXU1Mhut0dCMJANO9eulyTt3pTZpBi58PwVC7Tj47VmlwEAwH4lrRu2GhoaVFtbq2AwKI/HI5vNptra2j5TwzqdTtlstj6TDzQ1Nam2tlY2m02NjY0KBoOqrq5WS0tLWn1iASvqaN2kN25JPBQcAADIXFqTFEhSdXW1qqurk25TXl6ecBrYdPYH9jdFo0aZXQIAAPuVtFpeAfRPwfBhZpdgKflwox0AIL8RXpFXQt3d+9X4qISx9C2+9it6aM4JZpcBAMhzhFfklafOvUL3DJ9idhnZ091tdgWWseLv92lbywoCPwAgKcIr8sra/zxldglZ1bV9h9klWE6oiyHGAACJpX3DlhnCY8C6XC65XC6TqwEy17ltu9klWE53Z6cKhtFXGADyldfrldfrNe38eR1ePR6P2SXAovLlo+eurdvMLsFyund3SnkwSEN3V5eMggIZBXxABQDRohsVFy5cOOjn568y8tKGxUv6tZ9hGFmuZGC6aHnNWL5Mq3vPsMl68YtfN7sMAEAvhFfkpfd/N/jv5HKhk5bXjOXTtLqBO/9hdgkAgF4Ir8hL3bv7N1xWXnQXMAwVFBdr3OGHEF77Yee69WaXAADIY4RX5JXhJTZJ0ugZ0wZ0HLO7DxxZ/TVNOGke3Qb64bF55WaXAADIY4RX5JWJpxwvSeraudPkSgZgb+vvsAPGasfqj9X6st/kggAA2H8QXpFXQnt6BvXvaG0zuZIBMgyNnDpZkvT4iWfxUTgAAFlCeEVeCe3ZI0nqaN00sOPkQd/XUucxke8/+L9FJlaCTCX7/Vlxd4NW/uOBQawGABCN8Iq80r13dqUNz72o9jfeNrmagZly5mm6aOWrmnzmaVruuUvde4M5LCBBeG1d0qzF11yn5690D3JBAIAwwivySigq4D170bUZ72/2jVq9jZk5XYd+7f9px6o1eulL1+dFi3A2hLq7tW3lR1k95rBxB0iSJp56YlaP2x/xrlMoFNLjJ51tQjUAgGiEV+SV0J5ujThwkqT9Z4zUaRd+Rkd99xsK3PkPrX7oMbPLyYo3f3yb/jXbqa2Bldk7aD4F++7uvsvyqT4AGMIIr8groT17VHL0EZp78w3q2NSmPbt2mV3SgBmGoaNvqda4ww/R4muuU/ub75hd0oCt9T0nSdq5Zm32Dro3HO7p6MjeMfspFCe89m6NjbcNACD38jq8ut1uud1ueb1es0vBIAnt2SOjsFAHzLFLoZC2Bj7MbP98aR3r1X2hYNgwnem7X0Zhgd7++W9MKip7wg8vF893fyeoyKo0HtceKw/nBgAD4PV6IxnNDEWmnDVNHo/H7BIwyHrCa4HGznFIkrYub5HtiEMzPk6+9X2VpFEHTdGsKy/RikWN6tq5U0UjR5pdUv/tfX6z2foYDsLdu3dn7Zj9Ffdx9Qq0W1tWquToIwepIumd236vg86d36/XAwBkk8vlksvlkiQtXDj407nndcsrhp7Qnm4ZhYUad/gcyTDU/rq1Rxzobep589W1fbvalr5mdikDsuG5FyVJoc6u7B10bzbs7rBGeH35qzWDVI3UtWOH/DfcoqbTLxy0cwJAviK8Iq90d3aqYNgwDRs7VuMOP0QbX3ilX8fJm+4DvUw4uWcGsQ2LXza5kuzIxUxo+dDyGq/bQO/fqVF7J6EYDB2b2iVJe3bQVQEACK/IK92dnTKG9fRmmXbBOVr7xNPa2rIi7f3zsbtAtBETxmuMfZZe+86PI2PaWln3ruzdXBWK3LBlfngNdad+81NYXDwIlfTo2NQz41xB8fBBOycA5CvCK/JK9+5OFQ7v+Q/64AWflUIhrfm3z+Sq0pdOi6/981dIkj687185rib3ujuzeHNVHvV5jXvDVq9l4UA5GPbsfZMQYqILACC8IjPv/W6hnqv4Qs6OH93yOtY+S6NnTtfG51/K2flyJVkL8FHf+V+VOo9W8zdv0u7g5kGsKnuG28ZJkrqz2ed1r/xoeU3d5zX8Uf6g2FvPnp3WHzoOAAaK8IqM7Fy7XqseeixnH3lHt7xKkm3u4dr87vKcnMssBUVFOtHzS+1av0Hv/voOs8vpF6Oo5w1Grlpeze6znM44r60vLdWyO/46OPXsbXHNi1ZpADAZ4RUZGeuYpVBXl3as/jjrx25/4211tG6SUVgYWTbuiEO15f0P9ov+odHGlx2r6Zecr/d+dYc62oNml5Mxo6jnGmWz5TUSDkOh7Ibi/hWT1mYvX3dDjgvp0U13AQCIILwiI2PsMyX1jHGZbc9c+FlJ0o6oWZvGHXGounfv1rY0pyE1u8UuE0fffIM6t2zVu7/8k9mlZKxgb8trKMshM9wdoaN1U1aPm6l0ug2Etb8xCMO5MZsXAEQQXpGRsY7ZkpR2mMyEUdDz6xj+SFqSxh1xiCRp8zvL0jpG++tvpdymu7MzL0JuydFHakblhXrv13cM6s0/2ZCrbgMjJk+SJO1ctyF7x+1nLcmWHVnzdY3Z+1p4/KRzct4y+nbt73J6fACwEsIrMjJy6hQVDBumbTloeQ3P2mQU7LvZadxhcyRJm995P61DbHj2haTrQ6GQ7is9WEuqvtnPIlPIMBQfffMN6tq+w3KtrwV7b6rL6lSuoZBGTjlQUk/fajOl6vM6avpUXbhsiSZ98iTt2blTm99+L6f1rH3i6ZweHwCshPCKjBQUFmr07Bk56TYQ+Qg6KiQMGztWo2dMUzDN8JpK17Zt6tq2XR8s/HtWjpdQmuPN2o48TDMuden93/+fpUYeKMhFy6sUCa+7TG55jTvOa9TvZcHwYTIKCnTyX3taRNc//fxglQYAQ15eh1e32y232y2v12t2KYgy7vBD0vp4PlO79964VDR6dOz5jjhUm9/KTsvW7vb8C4hHfe8b6tyyVe///v/MLiV9e8N5tm/YGpnP3QaiFAwbJkkaY5+l0nnH6v3f/9+g3VTFzVsAzOb1eiMZzQx5HV49Ho88Ho9cLpfZpSDKxFOO19blAe3asDGrx+3cuk2SVDR6VMzyUufRCr71rrq2bx/wOfKxdbP02Lmaev5ZevdXd6hz2zazy0lPeFirLLW87trYqlBXlwpHjJAkvf79n2n1I//JyrH7I363gX3fh7tNGIaho268Xls/WKFV9w/Om+yu7TsG5TwAkIjL5YpkNDPkdXhFfpp46omSpI0vvJLV4051nS1JOvzbX41ZPuGU4xXas0eblr424HNsC3w44GPkwlHf+6Z2t7Vr2R/+YnYpaQllObw2Tjqs55uo7hbPuK7Wxhez+zuWthSjDUTfVDj94vN0wKEH662f/2ZQbgTcs4PwCmBoI7wiY+PLjlHBsGFqXdKc1eMOO2CsxthnaczM6THLJ540T1LqsJwqOKx5zKdnL752YEXmyMST5mnymafp1Rt/pE3+17XmMZ8Cf7/P7LJSCuVghq0T62/XzMsv0ojJkwZtHNXe4v4uRYfXqLGIjYICHXnj9Wp/9U0F/nZvzmuj5RXAUEd4RcYKR4yQ7Zgjtellf1aP292xW4XFw/ssLx5fqgMOm6MNKaaJTdUKuPH5JTE/58NwWdFOufMPGnHgJP234ot6+twr9MK1X9H7f/yL9uTjrEp7n7sda9Zq57osjgxgGJrj/pw++Y//0+HfvE7tr7+lXRtbs3f8dKUYV7UgquVVkuzXXq6Jp5ygpf/7Pa32Pp7LygivAIY8wiv6ZcIJTm165dX4g7n3U/fu3SooLo67btInT9LGxS8nvVklesD8zs1b+q7vFVZz8fHrQALxqKlTdPpDf4+ZpOGVr1brxc9/TY+fdLa68ujj4vDjXPXgo7p/ypFZO270IA22I3u6Emz9YEXWjp+ueNcxFNPyGvun0ygo0Kl336HRs2bouUu/oNWPPpGz2givAIY6wiv6Zfzxx6lz6zZtWdaStWPuSdDyKkkHfuoUdW7eomCS2Yyi73x/45a6vjd49Qrau7ds7X+xqaQ5VFZvE04sU9mvfhKzbOW9D6h1SbNe++5Ps1FZdmSx1TpR4B89a4YkadvKj7J2rnSlmmEruttA2JhZM3TWsw/LNvdwPXvRtfrgL3fnpLauHTtzclwAsArCK/pl/LxjJUmblr6atWN2d3SoIEF4nXTaKZKkDc+9mHj/Xt0GwqMXhPUOSTtWf9yfMnNuTtXn4i5/7zf1g1xJEr2ey4G0OG9cHNWdIyr0j5k5TZK0feWqfh+731KM8xovvEo909uWP/mgDjz9VL30pev1yvXfzXr3lGyMugEAVkZ4Rb8ccNgcFY4apbalr2ftmHs6dqtgePzwOnr6VI2eNUPrk8yg1afPa6/Wz96tads/XN2/QnOsoLBQF34wsJvTcq336bsH0C83PGZqb0WjR6t44gRzWl5TdRvo1ec12nDbOH36sX/qsOur9P5vPXrxC/+T1cDZuTmHnxgAgAUQXtEvBUVFKj1ublaHMuru6EjYbUCSJp/xCa1/ZnHCfq+9B8zv7uyKCRwFvVrLdre1D6Da3BrrmK1zX00yJWie3Wy2Z1dHv/eNGde31xuOMbOmm9Tymrwvd+8+r70VFBWp7Fc/0dybvq0Vf79PT55zuXbH6YfdHx2b2rJyHACwKsIr+u2gz5ypTS/7te3D7ISLPR2Jb9iSpMlnflK724Nqf+3NPuue+JRLj594dsyyB6cfLf+3b4r83Lu1rGNT/oZXqWfygtMfXhR3XVbv8O+P3je/7do1gENFtWj2Cq+jZ83Im5ZXJXkjFI9hGDrmhzfqE/9YqNaXlspXfol2rh/4zGH5/nsLALlGeEW/zb7qUhkFBfrvZV/KSoDtDG7WsLFjEq6fXP4pGQUFWv1w36GINjz3onbFCQbv/uqOyPdGwb5gVDB8eG5asLLcIjrNdY4ua2/R2EMcMcsfmDo3q+fJWJ/w2v+W12TGzJqurctatNxzp7q7sj+mbEIpbtgqOS79539m5YX61IN3avM7y/T0uVcMKOhL+f2JAQAMBsIr+m3M7Jn6xL0ebX77fb34+a8N6FhdO3dqx5q1Gnvw7ITbjDxwkg48/VStvPeB9Pt8Rm0X2rMvkIyYOF6724L9LTel3i2IAzHcNk4Xvr+kz/I3fnxb1s6RqT7Djg0kkEUfK07LqyQtqfqWlv1x8GYfiz89bE+dx/+hTsPGJH6TFc+088/WJ+6tV5v/Db1y/XczrmfKWWdIksbOsdNtAMCQl9fh1e12y+12y+sdnDnDkbmZl12kY37yHa1/ZrFaBzBpQXdHzw0/RWNGJz/flZdo6/KA2l59Q7uDm7X6kf/ojR/9Iuk+4SGxQlF9ZTu3blPLX+/Ryn8+2O+aB9uh/7Mg5uc3bvq5Nvn73jAXfPs9PVf5xdxObtArvHYPpOU1SXideMrxke9b/pr72avCkr056u/7kukXfEaHf/ur+sBzl9rffCejfQtHFKvkmKNUPL6UbgMATOf1eiMZzQx5HV49Ho88Ho9cLpfZpSAJxxev1rCxY7T069/p96QF4f1StVjOuOR8FQwbphV/v0++8kv0jOtqvXFzbdJ97hs3W1uWt8R87Ny5N9AG7vxnv+o1g/O2H/ZZ9ljZmX2WvXDtV/RR48Nqf/2t3BWTxW4DyYJi6bFzdeEHr8h52w/V/tqb2hpY2e/zZCTq9zhSXxa6hBx14/WSpGfOvyqj/UKhkGQYGl5qo+UVgOlcLlcko5khr8MrrGH4AWM17aJz1bqkWasefLR/BwkHg4Lkv5LFpSWaUXmBWv58t9qa0x+ma8uyFoW6+o5SkGpK2XxSOHy4Ltu8Qkfc8DWNmDwp7jahUEht/jckSUUjRw5abdnqNhDvzctYx2zNuOR8SVLLn/sO/B/q7taKe+9POvtaxiVFjfMa/h2JhNgBdAkpHl+q8ccfp+0frdau1k0ZFBSSjJ79c9ndBQCsgPCKrCi7/UeSpOcqvqCPH38y4/0jLa8pwqskHfHtr/aZgCCVN390mz5seEiSNGzcARo27gBJ1gqvUs8bBWfdLZr7g29HlkW3XEaPJ5rLsWD79nnNUstrgmA4ZvZMzbrqUr1d+1utevixmHXLF96lxVdVaXn9nf2uIU5R+77d+zuy6v5HJA18hqsT/vQLyTD03q8zmHQiFJJRUKDi8SW0vAIY8givyIoREyfoRM8vJUkfNj6c8f6R7gZptGqVHne0So7N7G77TS/7tfPjdRpeYtOla95U4cgRPeftzPId7IM0/urMin1daZbX/y3yfXSoH8jEASn1Dq87B3YHfTpOvOM2lRxzpJYs+Ka6duyILN+1bsPef7M3fFh095fw+MGvff9nkgY+ucX4smM18ZQTtObRprTfYIS6e7oNFI8vVde27bntzwwAeY7wiqyZs+BaTbvwM2r5890Z/+e6r+U1vY9kT2v4sw67vkrnv71Yh/1vVdrnKRw5QkWjR0fG6Qxl8aPmGFkcbSCeEZMmynbU4ZKkNY82RZZHz77UvTuHrcq9QtezF1+rLctbBnysZH2eh40dq+PqbtGuDRv12nd/qrdu/XV4p/6dN82aIn2l9y7LxkgSs664WO2vvalN6d7kGArJMAwNLy2RxEQFAIa2tMNrXV2dHA6HDMOQw+FQXV1d2ieprKxUWVlZ3K/GxsZ+FY78NOkTJ0pS3LFYkwoHgzS6DUjS2IPtmvfrn8p2xKEq++VP0j5Nwd6JCpJNhmAV0y46V5K0Y/XayLLOrdHhNXetc6FQSOMOPyRmWbivbT8Otu/7FMFw0idPkiS995t6vfbdn2jHmrVJt++v2JbXzthlab7BSmb2Zy+TJD1+0tkKhULq7upKerNj+Iat4vE94ZV+rwCGsrSSQmVlpWpqatTW1qaKigq1tbWppqZGVVXptXg1NjbK7/fH/QoEAgN6AMgvh33jOo2YPEmBO/+R0X6RG2TSDK/RDMPQ+W89n962e8Nr8YRSSVLrkmbL9XsNO/qWao2eMU27g5sjy3at3xj5fue6gc/mlFAopImnnqDLt6yILOpv14FM+uYWDh+uo39YE/n5gWlz9VGjN1xS1kQHyXDXkvDvaLpvsJIZPu4AjTviUEnSlvc/0D3DJuvp865MUlBPy2vx+J7fW1peAQxlKf8K+/1+NTY2yul0qr29XQ0NDWpvb5fdbpfH40kZPoPBoCSpurpaoVCoz1d1dXVWHgjyQ0FhoQ7+0jVa82hTZtN6pjlUViK2Iw+LhIFkjKKe7gJzb9p3w9M7t/2hX+c0W0FhobZ/tFrbV36kt3/xO0nSM66rI+ufv2JBol2zwzA0bOxYHfX9b0mSdm3YmGKHBKJDZxrXf+4Pvq1PPXhX5Ofg3jFT3/rJ7VpkTJDvzItjAn3/auo72kCmnw6kcsYj90hS5AbHpDc69mp5ZaxXAENZyr/C9fU9d8QuXLgw7vLwv4ksXbpUkuRwOJJuh/3HnKrPyTAMLb/jb2nvE8pCMPjMK026fOtKOb6QeAzNrct6+mVOO++syLKNi/vOXmUV4448TJL0anXfMWBzKbq19Ngff0eFo0bprZ/8UpvfX96fg0W+TefNi2EYmn7Rubom1KoT/ti3+9K6p/6rxZ+9rt9jDkvxb9iK1JmlPrZjZs/U2Dl2NX/j+6nr2RteR8+cLkl679d3pNgDAPZfKZOCz+eTzWaT0+mMWV5eXh5Zn0y4ZXbevHn9rREWM3r6VE274Bx98Oe70x//M9KfsP/htWjUKA0bM0Yn/+W3iWubMa3PsmwOKZXL4aniiX6sg9r9IaSYEBfq6lLn1m164bNfGbwaJM268tI+y478zv9qzSNP6O3axL8HKcVMK7xn76Ls3bAVdshXvph2PYZhqLi0RFPOOkMbnnvRst1dAGCgUiaFQCAgu90ed53dbk/ZbaClpaely+fzqaysLHLDV1VVVaRLAfY/h3z1S+po3aQPG9IbNmtry0pJ2QsGh349/pR1c2++oc+ybIaRXB4zngkn7HtTueH5l/qs35nF4aNi7A1TYc66myVJ2z9a049DRd+wldm+w23jYn6eU/U5HfvT72n6Refq9e//TMsX3pVgzxQ1dfcNr9l4g9XbxFNOSLOgUOTNwvSLe27U27WxNWt1AICVpPVXuLS0NO5ym82WMoCGw21NTc9NFhUVFZJ6pn6dPXs2AXY/NfnTn9TYQxxa9oc/p7X9k/N7WtDC07YO1PG/+Vnk+5lXXLxvRVRQGnnQ5J5zbts3sL8VnXpPT9edze/2/cj+5ev6hvVsCEWFKUk67PoqHffzm7Rr/YbM+5tmMNpAPKfeUy8Zhq7s+Fgn3nG7DMPQqYv+pMnzT9cS9zf1/NVVGQ/jFXPDVrjlNYOJNNJV6jw6vXqinu9R06dKkra8148uGgCwH0j6VzgcLG02W9z14VCbLICGw2tTU5Oam5vV0NCglpYWVVdXKxgMasGCxDeVzJs3L+bLrDl0kTmjoECHfuWLal3SrPbX38pgx+y1WNqvvbzn373DEkmxrXznvf6sbEcdrg3PvqCNL76StfMOtvFlx0iSXvlq35sfO9qDuTlpr5ZXSTrg0IMlKfPxXgfY1WL2lZfqmu6NKhw+PLKsaPRonfHIPZp78w36qNGrhw89SU9fcLXWPfVcetPIRncb2BtaM5lII10FRUWa/8xDadUTPu2BnzpFBcXFWvVQhsPRAcAAeTyePtnMDEnDazi0JgqnbW1tMdvF09zcrFAoFOkjG1ZbWyu73Z50nNelS5fGfLnd8T8KRn6adeUlkmFoVQZjvqY7SUE6TrnzD7om1Kqp587Xwf/vmp6FUS1qIyaM1/YPV0mS3ri5NmvnHWxjHLMTrtvw7Au5O3GvEDf2kJ6bMrcuz2z4u1CGN2ylq6CoSMfcUqOLP3xVc7//TbW+uFS+My/RA1Pn6uWv3KCtHySuM7bltTtcaOy/2aozKnRvW/Fh/HpCoUh3hWFjxmjqueVaec/96to5sKlqASATbre7TzYzQ1qff4VDam/BYDBpcE0lfBMYY73un0ZMmqjx847VGzf9XC9+6fq09snZ/U57/+PvfQd6+E7yYQeMzdGJc6+gsFCHXPeFhOtTDX7/YePD6bVGxu7YZ9FY+0zJMLRl2QBaXnPQV3jk5AN1zI++o4s/ek2fuNejCSc69cGf79bDh56kxdd+Je5EB/G6DeTKyMmTIt//y14W/6a/7u6YYH/Y/1apo3WTWv56b05rA4B8lDK8JrspK9nNXGHp9GlN1KcW1nfQuT0t7i1/uTu9HXKUXiP9FHsdf9S0KT3r947/OmCDPNpA2IGnn5pwXdf2+H16d3y8Vv4bbtZ/K7+o93+3MO42ifTu8ypJhSNGaPTM6Rm3vA6WopEjNeuKS3T6Q4t08Yev6vBvfUUf3veQHjrkRK24p9cnQHG6DcRblw1jZs/UkTfue3MXb5SEUK/RHSZ98mRNOGme3r3tD4M+wgUAmC1leC0vL1cwGJTfHzsHd3iIrN7dAaIFg0GVlJSosrIy7nq/3y+bzTag1lvkt6mf2ff78eqNP0q9Q67C697/+KPvIpekM30PSJK6tu/I9gmze7wUpp43P+G6RDekLb7mOr17+x8lSTtWf5zZCeP0eZWkA+bY867lNZ6Rkw+Us+4WXfDuCxo/7xgtvvrLev/3/7evpEFseZWk4279gWZUXCBJeu07P+4bSHs934ZhaPrF52rbig8TvjkBgP1VyvAangI2PFpAWG1tbcz6sOhW2vD4sI2NjX3Gg62rq1MgEKAf636udN6xke/frv2t9uzenXT7nLUiJWh5HTNzuqbMP13bP1ydm/MOkqLRozV2zr5PQeZ8+fOR71+78ccx2y657tta/egTWv90elPqxhWn5VWSxthnZvxc5qrPazrGzJ6pM59o1LQLP6Ol13933417ccZ5jfyco9/RU+7aN9Pb+md6XZs4z3fBsGF76+v/ZAwAYEUpw6vT6VRFRUVknNaqqio5HA75fD653e6YbgN1dXVyOByqq9s3601DQ4Mkaf78+Zo/f74qKyvlcDhUU1Mjp9MZCcHYPxUUFmrSp06J/NzRuin5DjkKBnO//01Nu+AczY4aeSCs7dU3FXzzHW2w8ExbknTmf3peayMmT9IJf9j3Ggzc9U9tWfZB5Ofld/xNz5zfaxayDJ/3eN0GpJ5xVzs3b8noWGZ1tQgrLC7WqX//o0ZOOVAvf6Vaoe7u2HFeu/u2guZC0ciROuPf/5Akrbg7thtDvOfbSNCPGwD2d2ndsNXQ0KDa2loFg0F5PB7ZbDbV1tb2mRrW6XT2mY3LbrerpaVFFRUVWrp0qRobGyP7Nzc3Z/fRIC8d97N901/u2pBiYPUcBYORkw/U6Q8t0vBxB/RZFw7UrRYeLkvqaUW8JtSqirXvyCgoiBlD9Onzrky675p/J58pL554raTDDhir7t27taejI+3jhEzoNtDbsLFjdezPvq/2197UuiefixmVok+3gRyG7XA3m5Y/9+ojHqebhlFYGL8+ANjPFaW7YXV1taqr+44jGa28vFzt7e19ltvt9kgLLIaeCSftGwcuVXg14+aTEZMmateGjSocMWLQz51L5zY/pUXGBEnS1g9WSEr8/G55b7m2fbhKY2ZOT+/gCY4zbO+bg87NW1Q4aWLGxxrsbgPRZl5+kZq/dZOW/emvmnXVvmlnzQqHuzZs1IjwcxivpTvcFYaWVwBDTPamigESMAoKNG/vjFdv3/rrpNua8RFo+VMPSpJe+Z8bB36wPL/zO9TVlXDdWz+5PYMDheJO5RoecixbM6UNpsLiYs264mJ9/PhT2rNzV2R579/JXL/BOv/N/0qSli/8e+w5+7S89vz5zniYMwCwOMIrBsUc97WSpPXPLE7+n78J4e+Aw+ZEvs/aoO8mtiBGO+XOP8T83J3khrlMQlmiPq/DDhgjSdqdSXjNg24DYZPPPE17du7Uppf3ja4ymN0GJMl21OE66JwztfxPf405Z59uA5GW1/x+wwQA2UZ4xaAoHDFCw0tskqTdwc2JNzQhvBYUFuroH/aMphG48x+Dfv5cir5BbcfadZFJGQYslKDPa1S3gbQPFX3JTQ6v4448VNK+bhZS37v5B6Nry6RPnaIda9ZGhsHqmWGLPq8AIBFeMYhO+NMvJCUfU9SsO6eLx/dMlPHydTdoV+smvfL172jFvfdrxb33m1JPthiGobJf/USS9E7d77TW92zijTPIZIlbXvvRbSCPulqMnj5VkrRt5UeRZWaEw1FTeybP2PHxup4FvWbYkhhtAMDQldfh1e12y+12y+v1ml0KssB21OGSpE2vvJp4I5NyTHiM1ILiYn382JN6/3cLtfiqKi2+qirFnvnvsK/3jKX83q/r9d/KL6bcPtTdrZe/VqPgW+8m3zDeUFnhltct29IvME9u2JJ6PiEoKC7WlveW71uY4xm24hk1dbKkfW/04vd5peUVgDm8Xm8ko5khr8Orx+ORx+ORy+UyuxRkwbjDD+kbDCTt2rhvBAKzpro86KwzJEndHR3q3BobvKLHSLUio6BAR1T/T8rtWv5yt7o7O7X9w1Va9oc/65kLrkm8caLRBsItrxl1G8iflldJKi61xfzc54aoQah39KwZkqQVi/aO99prelhJMgrCs8bR8gpgcLlcrkhGM0Neh1fsX4yCAo2eMVXbP4qdgemZCz+77wcTg8zMKy6WJL3y1dgh4R4+9CSFurvTuqs734JY2LE//Z5Gz5iWcrs3bqnbF4aStYImmB52wKMN5MGNbnNv+nbMz4uvqtLqR58Y1BrG2mdp2AFj1dG6Sa/94Fa1v/5WknFeCa8AhhbCKwbViIkT1NHaFrNs6/J9Uwqb2Yrk+MJVCdc9V/lF3VN0YNrHMvvj794Kiop0rv8pjTvysKTbbX7n/X1vIJI8hkR9XguLizW8tESb312WfnF5NNqAJB1w6MF9lr3/24WR7wfrDcq0C87R6ocf7xnCjBm2ACCC8IpBVTyhVLvWb4xZNuLAfYPZH3TOpwe7pIjRMxO3TK564JFBrCQ3iseXqnhCadJtgm+/F2lhNgoyb3mVJNtRh2n7qjXpF5ZnrdXj5x0rSZpc/qnIso2LXx70Og79+oLYBb2eJ/q8AhiqCK8YVOOOOFSb31uurh07IstKjj5CknTyX36r8WXHmlSZNO7QObLNPSLpNt1JBvm3guNu/UHS9VuXB9T+6ps9P6ToNpBofeGIEeruSDyebN9DRd+wlfZuOTNs7Fi53ntRJ//1dzqu9iYd+nV3ZMgqSYMWticc74z5edvKVbEbMMMWgCGK8IpBNfHUExTq6lJr1CDwRWPHaMTkSUk/th8sh12f/M7JN3/0i0GqJDcmnny8Sp1HJ93m+St7noOty1qSHyxB0iwoHq49HR3pF5Vn3Qaknjcyo6cdpCOrv67Dv3ld7MpBbCg+5+UnVFBcLEkKvvF2zLpwyzgzbAEYagivGFQTTzlBkvT692+NLPvAc5d2rdtgVkkx7NdennT9mz++PbPZo/LQWc/tG3queOIEXb1ngz7xj4Vxt93Vuinu8mT9PguLi9W9K/3wmq83uYWNnjFNw23jIj8PZr0Tjnfqql1rNOuqSzXvt7fGrAt3G6DlFcBQQ3jFoCouLZEkbVy8JHtTsWZRwbBhOnXRn2T//JUJt+k9GkGMPA9iklQ0enSkNXHSqSfIKCjQrMsv1oxL+w5J193ZGf8gSfq89rS8pt9tIEaetLxGMwxDtqOTdyfJtU/cXa/D/ie2D2zkhi1GGwAwxBBeMejKn3xAkvTObX8wuZL4Zl9dKWftTQnX79rQqvXPLlbn1iQtsPmXwWKU3f5jne69Wyffue8aHPLVvhMYJOy7mqzPa3GxuvvZbSDfRmkIKzl27r4f8uQNSuSGLVpeAQwxhFcMusmfPk0HfaZc7/3qDu0Obja7nLhGTJqoOVWfi7tu24oP1XT6hVp6/fcGuarsmnb+2Rq+d1xWSZp8xif7bJOo72qiobIkqTDTltc8CYPJlB6Xh+E10vJKn1cAQwvhFaY4/Btf1u72oO4rcZhdSkLH/Tx+62t4XNpdGzbGXW9lpz1wZ8zP21pWKvj2e337eYZCCT/hL55Qqt3twdg79JMI5eENW71Fh9d86aNrFPb8+V55z/0mVwIAg4vwClNEj6EpSc7bfmhSJYkNt41T5ablCdePnWMfxGoGx4yLz9M1oVY5f3GLJOnp867UI0d9QoG7/tl34wRB84DD5ii0Z4+2r/o4vZNGhcGNzy/JtORBMe6IQ80uoY9S5zEae4hDLX/7x375RgoAEsnr8Op2u+V2u+X1elNvDEsxDEPnLNk35eawsWNMrCax8A1m8QyL+sh9f1M4alTMz9GzoKVStHffPenekBfVkLnj43Vpn2cwFQwbFvk+X/rlDreN0+kPLdKenTu13HOX2eUAGEK8Xm8ko5khr8Orx+ORx+ORy9X3LmhY34QTnBp//HGSeoYjyleXb10Zd/mbP7qtz934rUuaB6Gi3Ivp4ympcOSIyPfhG4TCfS57KxzRMy7pngyGy4rI45uPrtixSge7r9XRP6wxu5SIcYfN0ZSzP63Xf3CrHjthvtY/94LZJQEYAlwuVySjmSGvwyv2f2c8eq9O/tvvNeWsM8wuJaFhY8bo04/F+dhcUsemtsj3Oz5eK9+nL5YUNQanRU08+XgdcOjBkZ8/un/fpx/hwB7dGhktHHTTbXmN7kOazwPuF40cqZPqf5m0Nd4MJ95xmw7/5nXqaG2T74yL9PottTG/lwCwvyG8wlQjJk6Q43NXJGzFyxcHnXOmrgm1quyXP45Z3rVjX0Dr3Lot8r3Vw6skzb35hsj37a++GelXGQ6vxrCiuPsVjugJr5uWvpbeiaLCK2OWZm7MrBkqu/3HOu+NZzXrqkv15g9/oYaJh+qJ087XO7f9Xq2v+C0/rTEARIv/vw+AuEZNOyjm566owBptfwivsy6/WIuvqor83Hjg4bpo5auRvr6JWl61d9rSV2t+pCOrv576RDHhNX9bXvPdsDFjdMpdf9ShX1+gNY826aOGh+W/4RZJPW8oxh1xiA44/BCNmjpFow6arJFTDtTIgyZr9MzpGjV1St6/gbSSZXf8VRNOmqfSY+em3hhAxgivQAamX3Supl90rlb969+SYltboxUUWT+8GgUFcr3zgrxHnBJZtvjqL+tTD/YMp5UovI46aErk+0XGBF2wbIkOmJN4SLQQ4TVrDMPQhOOdmnC8U8fcUqMda9dpw3MvqnVJsza/9Z42Pr9EO9euV/fu2HF4C0eN0ljHLI2xz9SoaQdp1NQpGjl5kkZMnqSRkydp5JQDVTxxggrSfFO2/aPVGjF5kgqHD8+o/j0dHercuk0jJozPaL988/J1PZ9aXBNqNbkSYP9EeAUyUDBsmD714F1aZEyQJH2w8O+a9ImTJMXehb4/tLxK0rjDD9FpjX/VcxVfkNQzrW/nlp6ZxYwEAX3U1CmaedmF+vC+hyRJb9f+Vif/328SnyQ6vObxDVtWNGrKZM26/GLNuvziyLJQKKTdbe3a8fE67fx4nbat+FBb3v9AW1tWalvLSm149oW4k4cYBQUqnjghEmptRx2mSaedrANPO0XDbeMi2/mrb9E7v/i9JOmK7R9FRp9IpHvPHq168FFNv+hcPXfp57Xm0SZCn8nWP/eCFl/9ZbneWaxhY/ffUVVgXYRXoB9O+NMv9PJ1Nyhw1z816bSTdfCXrolZv7+EV0macalLR99SrTduqZMkPfWZyyVJHa2Jbwo66c+/0ezPXqZnXFdrx0dr0j7X8HEHDKxYpGQYhorHl6p4fKlK5h4Rd5uunTu1a/1G7Vy7XjvXbdCudT3/hn/e+fE6vf/7P+vd2/+oguHDNafqczr8m9dpzKwZkeAqSf8YPUOjZ07XiZ5fasr80+MOM/bWT3+pN26u1WkP3Kk1jzZJkvbs3h1ptd29Zat2t7WrcESxhtvGRfpUW8HWlhUa65htdhkZe+27P9WO1R+r/fW3I2/O9xc7163X4yeerTMe+6dseTh+M9JDJyegHw758hdUUNwzJNTbP+9pVYy+2Wh/Cq+SNPemG3TsT3umww2P+dp7mLBow8aM0bTzz9YhX/2SNix+WV3JRh6Iank90fPL7BSMASkaOVJjZs3QxJOP14yLz9Mh131Rx/zwRp3k+ZXOePhunbv0SV0ebNH8Zx+W/drL9P7vFupfs516+avVkWPMvfkGjZ4xTds/XKWnzq7U4yeepc3v9530I/jGO5KkUNTv045Va/TmT27Xnt279ciRp+pfs526f8qRajrjoj77b/jvi1pkTNAr1383+09EP0R3g3nhc18zsZIB2PsmY/fmLSYXkn2rvf/R9o9W693b/mB2KRgAwivQT5dvDmi4bZxsRx8pKba/ZqKP1K3KMAwd/P9iW5dDnanvYJ963nzt2bFDGxe/nHCb6P/siy3e13EoKRwxQgeedopOWvhrnfPSfyRJy/74l8j6Y26p0cUfvqYrd67WifW3a2vLSj1+wllact23Y4ZEi1z/qBvGXlrwDb3+g1u1+uHHtWP1vpnaWl9a2qeOt2t/K0l6/7cebf9odVYfY39E/x3o2r7DxEr6z9h70+Uz51+V/I2nBYWH8tvfHtdQQ3gF+qmwuFgHnn6qVj3wiHau3xAzHNH+1vIqSSMmTdQJf/pF5Oc9vW76iWfCCU7JMLT2iacTbxQVXgsK+ZNkRRNOLNPlW1dq+iXn91lXOGKE5rg/pxmXnq/OLVu1/I6/KXDnP5Ieb/3Tz0uK37q/8p8Pxvy8u31f/9wtGcwENxDde/bEvOmKFor6O5BqrONQd7e2rfwoq7VlRVT3jq3LWkwspEfrK369/NXquC33meju6tILn/2KJGnPzl3ZKA0m4X8KYABmXXmJJOnjx56MaXFJ965sqznky1/QnKrPSZK6dyfuNhBWPL60J+A/9FjCsUajQ8D+GPqHimFjxuhT9/9Nc778edni9KWNblV/6UvX71ux9/rHazWNHqot7PkrFqhrx74WzejWzW2Blf0pPSPde/bonqID9WrND+Ouj/47kCogNX/rB/rXbKd2rt+Q1RoTabnzH1ruuTPldtHDpuXDhBePn3CWlv3xL5FJYPor+neF8GpthFdgAGZUXKCC4mK9cUudQl1R3Qb24xB2xLe/KkmafdWlaW1/yHVf0NZlLXrpS9era/v2vhsQXvcrJ/7pNp3/xnN9lg8bOybm5y3LPoj52f+tm9I+x38v/3/q7urSznXr1f76W5HlS9zf1KqH/p1hxZnZ3R6UpJgb06J1R/0d2P7Rar38tcTTCa+8t6cVefM77yfcprurS89d9iWtuKexH9XGevHzX9OSqm+l3C42vLYP+LzZsvPjdQPaP/qNBSObWFteh1e32y232y2v15t6Y8AERkGBJpzo1PYPV2nL8n0fr+3PIWzswXZdE2rVhBPL0tp+ZuWFcnzhKgXu+qcaDzxCwbffS7wxA+Xvtw673h3z88OH7r2LPcHH7/E4b/+RJGnNI0/onmGT5a/u2/r57EXXxrwWsy16lI23637bZ32o1ycMy/7wZ21fFX/EjZEHHShJ2vL+B3HXSz0jFnzU8JBe/OL1CbdJR3RY29PRkXzjqG4D/73sS9oYp6+xWeK+AU5TdDeUUufR2ShnyPJ6vZGMZoa8/p/C4/HI4/HI5XKZXQqQ0Ml/7hlt4O2f/TqybH+7YWugTlz4Kx31vW+qa/t2PXLUJ9R8w837ugtEt7wSXvdbRaNG6cpdsSHuzZ/+MjLhRzoOOudMnfzX30V+XvH3++Ju9/AhJ2rNY77+FZpCx8Z9Y9C+WvMjrXvm+cjv8ju//KMeO35+n30enHGMPv7PU32Wh1ujt7yXOLx2bNwkSeru6BhQa2Hntn2hb2uKvsF7dsWG2/+c8hltan6t3+fOpn+MmdnvANsddZOps/bmbJU0JLlcrkhGMwP/UwADNPZgu4644Wva/O6yyDJmiopVUFioY3/yXZ266E+SpHdv+4Oevfha7drYGtvnNc44oNh/FBYX68qdq1X+5AOSpNe//7OM9h82ZrQcn79S14RaI32vw0YeNDlmqLWnz71Cb9366wHXLPX0c1331HNaZEzQE6fFNqb4zrhIS6q+qVB3t/zfuknbVnwY9xhPnXOZ1j0V252ia2+gfO839dq+ak3cfuG7NuwLy/855TMJbxRLpTNq4olH5n4yafeK8EQkEaGQHptXnrAFebCtf/aFfu0XSjK8H6yF8ApkwdE/jO3XlmwA/6Fs9tWVunzrSjm+cJXWPPKEvEd+QoG/Jb/zHPuXwhEjNPnTp+nMJzLvw1k0ZnTk+943hRWOGKGD/99ndcW2D3XqPfWSpNe++xO9+ZPbB1awpIfnnCDfmZfELLuwZak+9dDfJcPQBwv/rvunHJnyOL4zL9FL7m9Eus60vfpmZN2DM47Rv+xlCr71bsw+u6JaeluXNOshxzxtWvpq6o/+e2ld0hzz87MXXaunz79SO9au6xuI97bwjnHM1sELPhtZ/J9TPqN3bvu9Nr+7rN8hOhuePu9KPTD96IxrCHcbOOjc8lyUhUFEeAWyoGjkyJg/iB1t+XOTQ74ZNmaMTv7Lb3Xea89o9IypWvdk35t7sP+bMv90TTj5+D7Lx/Wa9eio730z8n10eJ1+8bkx2xWOKJZhGCoaPVqzr7xUlwV7Php//Qe36smzKwc0Bmy81tTi8aWafsFndMX2jzTrqku1a8PG2Lq/+w2d/vAiXd29UZdvWaG5P/iWSp1HK3DXfXrkqE/ov5d/SQqFNLy0RGW//LFK5x2rzs1b9OhxZ+ixE8/Ss5d8TpuWvhppeT1x4a9UNHq0tq34UI8dP1/3Tz5Cz1/zZb32g1v1/h//osCi+7TqX//W2qZntGHxEm3yv67N7y6LDGn32nd/IkmactYZsn/uCknSmkeb9MBBR+nxk87We79bqB1re26ImnL2GZKk8159Sid5fqVrQq0q++WPJcOQ/4Zb5D3iFD182El67Qe3as1jPm3yv66tgZXatWGjOrdtSziyyEBd8vFbkd+BHas/1t0FE7X1g/SHRwt3G3DsffywLiNk5tunJAzDMPWdHZCpzm3btNxzlza97NeJd9weM9874gt1d+u+0oPVuXcmH+a0H1p2bWzV6z+4Vcvr9w3fdMQNX4u5k//E+tsjd8j3/v1YZEyIfF9y3Fyd53864XpJOvxbX1HZbT/KuM7ex5GkK3etUeHeWfb27N6te4sPill/6P8s0PG/vbXPfjvXrdczF1yjTa+8Kkk6+kc36ugffFuStPndZXr3V3/S1g9WqP31t9W5ZatCXV0aPXO6Ll75qtpff2vvurcUfPt9tb74inat35i0L+yoaQdpmutsbftwlT7+ty/mOVzre1YfNjykjx9t0o41ayVJR954vbavWqPVDz2mK7bGhvZQKKRNr7yqtubX9FGjV+ufWZzw3OGbVkN79uiAQw/WcNs4GYWF2tPRIaOgQMUTxmvYAWNkm3uEhh0wVluXBzThpDJNPXd+n7+ddxcdqMO/9RU5a2/Wnl271LqkWU2nXxhZP+vqCh3y5c+r5NijtDu4RaOmTonbBant1Tf0b+en9akH79L0i87tsx79Y0ZeI7wCMNWuDRu1rP5OHXXj9SoYNszscmCCzm3b9M+xsyT1hMIP73tIKxY1qPS4uRp7iCMyLmyy8DrjUpdOa/xrzHrfmRdr3VP/jVl2wh/rdMChB2vyp09Lu7544fXq7o0xAelfBx+vbS0rdOR3/ldv3/prnXLXH2X/7GVxj7dz/QbdP7mn28MFy5bogDmOPttsWd6iV75ao7VNz0hK/Mauu7NTHZva1Ll1m7q271DX1m3q2rlLe3bt0s6167XqgUdjJglJdJx1zzyv5m/8QO2vvZly27BdrZu0dVmLdm1o1e7NW9S5Zas6t2xV1/Yd6mjdpJ1r16ugqEjS3vFiDUMdm9plFBRoz86d2v7RGnXHmexkqutsTZl/uorHl2isY7YeP+lszb3p2zrmhzfGbLf60Sf0as2PtOW95TH3GYw4cJImffIkTTn7DE2/4ByNmDRRoVBIH/x5kZYs+IZOf+QeTTvvrKSPDekjvEYhvALA0LH53WUqKB6usfZZMcsDf79PL1z7Fc3+7GU69a4/xqzbsuwDhfZ0a8PzL2nmZRdp+LgDYtbv6eiQ79MXa+MLfacnHn/8cSo5bq5OvOP2lDcKxguvvYPdU5+5XB8//qTOe/1ZjZp2kIaX2JIe9582uzo3b9Gl697RyAMnxd0mukV3IJ9KbFr6amQUhGTHCXV3a93Tz+vJ8ksGfM50dHd1qXPLVm14/iWtevDfCvztXknS8BJbZDzdMMeXrtbJ//ebuMfZuX6DVj3wiDb89yVtan5dRkGBtry3bzauUVOnRFqWJemMR+/V1HP7jgqB/iG8RiG8AgD27Nqll79SrWN/9j2NnHxgxvtHt+pmU+9gt+PjtVr1wKM65KtfSmvUjOA77yvwt3t1XO3NSbd/9cYf6cBPf1IHnXXGgOoNB/B0Aul7v6nX+ude1Kfu/9uAztlfoVBIO1Z/rO0rP9K6p/6rN26p0/gTnPrMkifSPsam5tf09s9/q10bW9WxcZO6u7q0dVmLSo6dq9O9d2v0tINSHwRpIbxGIbwCALJh3dP/le/TF8v+uSu04b8vqXPrtpjxWvvDav2zO9raJcNQcYnN7FIyEuruVvO3b5LjC1epJM60wzAf4TUK4RUAMFg6t27VWt9z2t0e1Ks3/jhhuD2u7mYF33hHp/79T4NcIZCfCK9RCK8AADPtDm7Wyn8+qJe//O3Isqv3bGAmOCAK4TUK4RUAACC/mZHXigb1bBlyu92SeubQdblcKbYGAABArnm9Xnm9XtPOT8srAAAA+sWMvEbHHQAAAFgG4RUAAACWQXgFAACAZaQdXuvq6uRwOGQYhhwOh+rq6vp90sbGRhmGIZ/P1+9jAAAAYOhJK7xWVlaqpqZGbW1tqqioUFtbm2pqalRVVZXxCYPBoBYsWJDxfgAAAEDK8Or3+9XY2Cin06n29nY1NDSovb1ddrtdHo9HgUAgoxMuWLBAwWCwv/UCAABgCEsZXuvr6yVJCxcujLs8/G86Ghsb1djYKLvdnkmNAAAAgKQ0wqvP55PNZpPT6YxZXl5eHlmfjnB3gfLy8n51NwAAAABShtdAIJCwpdRut6fdbSDcXaChoSGzCgEAAIC90rphq7S0NO5ym82WVv/VcHeB+vp62Wy2TOoDAAAAIoqSrQwH00SBMxxqg8Fgwm2iuwu43e6Mips3b17Mz263O+NjAAAAYOA8Ho88Ho/ZZSQPr+FAmqh1ta2tLWa7eMLdBTK5sSts6dKlGe8DAACA7IvXiGgYxqDXkVa3gXBI7S1Zi6vUczNXuLsAIwwAAABgoFKG12Q3ZSW7mSu8XpKqqqpkGEbkq6amRpI0f/58GYahxsbG/tQOAACAISZptwGpZ0gsj8cjv98fM1xWeIis8JBZ8djt9rh9VJcuXSq/36/y8nLZ7XZaZQEAAJAWIxQKhZJt4Pf7VVZWpvLycjU1NUWWz58/Xz6fTy0tLTHhM1VrrCTV1dWppqZGTU1NCcOvYRhKURoAAABMZEZeS9ltwOl0qqKiQj6fT2VlZaqqqpLD4ZDP55Pb7Y4JqnV1dXI4HKqrq8tp0QAAABia0rphq6GhQbW1tQoGg/J4PLLZbKqtre0zgoDT6Yw7GxcAAACQDSm7DZiFbgMAAAD5LS+7DQAAAAD5gvAKAAAAyyC8AgAAwDIIrwAAALCMlJMUmCk8wYHL5ZLL5TK5GgAAAHi9Xnm9XtPOz2gDAAAA6BdGGwAAAACSILwCAADAMgivAAAAsAzCKwAAACyD8AoAAADLILwCAADAMgivAAAAsAzCKwAAACyD8AoAAADLILwCAADAMgivAAAAsIwiswtIxu12S5JcLpdcLpfJ1QAAAMDr9crr9Zp2fiMUCoVMO3sShmEoT0sDAACAzMlrdBsAAACAZRBeAQAAYBmEVwAAAFgG4RUAAACWQXgFAACAZRBeAQAAYBmEVwAAAFgG4RUAAACWQXgFAACAZRBeAQAAYBmEVwAAAFhGkdkFJON2uyVJLpdLLpfL5GoAAADg9Xrl9XpNO78RCoVCpp09CcMwlKelAQAAQObkNboNAAAAwDIIrwAAALAMwisAAAAsg/AKAAAAyyC8AgAAwDIIrwAAALAMwisAAAAsg/AKAAAAyyC8AgAAwDIIrwAAALAMwisAAAAso8jsApJxu92SJJfLJZfLZXI1AAAA8Hq98nq9pp3fCIVCIdPOnoRhGMrT0gAAACBz8hrdBgAAAGAZhFcAAABYBuEVAAAAlkF4BQAAgGWkHV7r6urkcDhkGIYcDofq6urSPkkwGFRlZWXM/jU1Nf0qGAAAAENXWqMNVFZWqrGxUTabTeXl5fL5fAoGg3K73aqvr0+6bzAY1OzZsxUMBlVeXi6bzSa/369AICCn06nm5ub4hTHaAAAAQF7Ly9EG/H6/Ghsb5XQ61d7eroaGBrW3t8tut8vj8SgQCCTdv6amRsFgUA0NDWpqalJDQ4NaWlpUXl4eOTasxePxmF0CkuD65C+uTf7i2uQvrg16Sxlewy2rCxcujLs8Vcurz+eTzWZTRUVFzPJwt4FXXnkl/WqRF/hDkt+4PvmLa5O/uDb5i2uD3lKG13D4dDqdMcvLy8sj61O57LLL+iwrLS2V1NOtYLBkezaIXMwuYZVjZpsVHrcVaswFqzxuK1zvXLDC47bKMbPNCo/bCjXmghUetxVqNEvK8BoIBGS32+Ous9vtKbsNtLS0xG2dDYfesrKydOrMiqH6i2WFX1YrPG4r1JgLVnncVrjeuWCFx22VY2abFR63FWrMBSs8bivUaJaidDYKt5L2ZrPZUobXaD6fTw0NDVq6dKn8fr8qKirkdrvT3h8AAABDXCiJ9vb2kKRQRUVF3PXl5eUhSaH29vZkh4lwu90hSZGv+vr6hNtGb8cXX3zxxRdffPHFV35+DbakLa82m01S4n6pbW1tMdulUl9fr/r6egUCAVVVVamqqkotLS2qra3ts22IYbIAAADQS8pxXg3DSDgeq8PhUFtbm9rb2/t18pKSEknq9/4AAAAYWlLesJXspqxkN3NJPWPEVlZWJhyRwG63D+poAwAAALC2lOG1vLxcwWBQfr8/Znk4kIaHzIrHZrOpsbFRDQ0NMcsrKytVVlam119/XYWFhSorK4t8xZu0IJOpaQcyjS2S47kdPOHXSLwvXiODy+PxRD4lSiRXzz/XKrlU14bXkXlqampinp+qqqqEjVW8fgZfutcnb19DqTrFNjc3hySFysvLY5aHb9ZqaWmJWd77Z7vdHpIUam5ujixTkk6/tbW1MftXVFSEJIVsNluooqIiZLPZQpJCbre7T62ZbIvM8NwOLl4j+cPpdIZsNlvC9bl6/rlWqaW6NryOzBH+f99ut4cqKipCTqcz8nz1vsGb18/gy+T65OtrKK1bxMIncTqdIbfbHXngvU9SW1vb5wE1NTVFHmh5eXnowgsvjPxst9uTjlQQDs5OpzNmefj80UE5k22RGZ7bwRUe5aO6ujrltrxGcqO9vT3U1NQUeZOeKCDl6vnnWiWW7rXhdWSO+vr6uPkgvDy6IYzXz+DL5Prk82so7fENamtrIwd2Op19Enco1BNUbTZbqKmpqc+DKi8vjyRrSaGzzjor5TnDQ2tFt9qGz9P7Cc1kW2SG53ZwhZ/XZEPJhfEayY3eLQyJAlKunn+uVWLpXhteR+ZINoRmOEOE8foZfJlcn3x+DQ364FzhdN+76HjsdnvCP0y9U3sm2yIzPLeDi9eI+RoaGiJfNpst4fOWq+efa5VYuteG15E5bDZbyG63x13Xu7shr5/Bl8n1yefXUMobtrKtpaVFUs8NX2VlZUk7C2cyNe1Ap7FFYjy3g4vXiPkqKioiX4lmGJRy9/xzrRJL99rwOjLHk08+qaamprjrli5dKkmR54/Xz+DL5Prk82to0MNruKiamhpJPX+IpJ67RmfPnt3nCUk2Ne1AtkVmeG4HD68Ra8nV88+1GhheR+ZwOp1xg0k48ISvQxivn8GVyfXJ59eQaeG1qalJzc3NamhoUEtLi6qrqxUMBrVgwQJJ+2b1SjR7V/iBB4PBjLZFZnhuBx+vEWvI1fPPtcoOXkf5IRgMqrKyUh6PR3a7XQsXLowsl3j9mC3R9ZHy+zWUdHrYXIg3U5ck1dbWqrGxMTJuWH+mps1kW6SnP9cBA8NrxBpy/fxzrQaG15H5PB6PqqqqJPWMCd/Q0BB5bnj9mC/Z9ZHy+zU04JbXkpISGYaR8iudwWedTqckxfR3CD+Q3oLBYJ8Hl8m2yAzPbX7gNZJ/cvX8c61yh9dRbgWDQc2fP19VVVWy2WxqaGhQU1NT3OeG18/gy+T6JGL2a2jALa+Jknlv0U3BqQoLb5tqatrwk5fptsgMz+3g4jViHbl6/rlWA8fryDxnnnmm/H6/Kioq+sywGY3XjznSvT75/BoacMur3W5P6yvcCbekpESVlZVxj+X3+2Wz2SJPViZT0w5kGlskx3M7eHiNWEuunn+u1cDwOjJPTU2N/H6/qqurkwYjidePGdK9Pnn/Gko5mFaWhach6z2RQXh2rujBaTOZmjbTaWyRPp7bwcVrJL8kG5MwV88/1yo9ya4NryNz2JKMvdsbr5/Bl8n1yefX0KCH15aWlsjMKOXl5aGKioqYmbt6S3dq2ky3RWZ4bgcPr5H8kiwghUK5e/65Vqkluza8jgZf+Dm32Wwhp9OZ8Csar5/Bk+n1yefX0KCH11Co5wmpqKiITBebaLrZsHSmpu3PtsgMz+3g4TWSP1KF11Aod88/1yq5VNeG19HgCk/vmeqr99SkvH4GR3+uT76+hoxQKBQSAAAAYAGDPkkBAAAA0F+EVwAAAFgG4RUAAACWQXgFAACAZRBeAQAAYBmEVwAAAFgG4RUAAACWQXgFAACAZRBeAQAAYBmEVwAAAFjG/wc2Z5GQP9sOiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 799.992x599.976 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# repeatedly learning on same set of training data to make sure loss goes down\n",
    "# temp_loss = []\n",
    "\n",
    "for x in range(1000):\n",
    "    temp_curr_loss = update_step(input_tr, target_tr)[1].numpy()\n",
    "    print(f\"Loss: {temp_curr_loss}\")\n",
    "    temp_loss.append(temp_curr_loss)\n",
    "plt.plot(temp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "facial-efficiency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing update_step\n",
      "Tensor(\"add:0\", shape=(10, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/home/users/mfong/anaconda3/envs/graph/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing update_step\n",
      "Tensor(\"add:0\", shape=(10, 1), dtype=float32)\n",
      "Epoch: 0\tLoss value: 0.5665999956491614\n",
      "Epoch: 1\tLoss value: 0.5501471288528177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-a35e825d1339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtarget_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mupdate_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mnum_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/graph/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data / training parameters.\n",
    "num_training_iterations = 20\n",
    "\n",
    "\n",
    "# for epoch in range(1000):\n",
    "#     total_loss = 0.\n",
    "#     num_batches = 0\n",
    "    \n",
    "#     for _ in range(num_training_iterations):\n",
    "#         input_tr, target_tr = next(training_data)\n",
    "#         total_loss += update_step(input_tr, target_tr)[1].numpy()\n",
    "#         num_batches += 1\n",
    "        \n",
    "#     loss_tr = total_loss / num_batches\n",
    "#     losses_tr.append(loss_tr)\n",
    "#     print(f\"Epoch: {epoch}\\tLoss value: {loss_tr}\")\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0.\n",
    "    num_batches = 0\n",
    "    \n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for data in train_graphs:\n",
    "        input_tr, target_tr = data\n",
    "        if input_tr is None:\n",
    "                continue\n",
    "        input_list.append(input_tr)\n",
    "        target_list.append(target_tr)\n",
    "        if len(input_list) >= batch_size:\n",
    "            input_tr = utils_tf.concat(input_list, axis=0)\n",
    "            target_tr = utils_tf.concat(target_list, axis=0)\n",
    "            \n",
    "            total_loss += update_step(input_tr, target_tr)[1].numpy()\n",
    "            \n",
    "            num_batches += 1\n",
    "            input_list = []\n",
    "            target_list = []\n",
    "    \n",
    "    loss_tr = total_loss / num_batches\n",
    "    losses_tr.append(loss_tr)\n",
    "    print(f\"Epoch: {epoch}\\tLoss value: {loss_tr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fuzzy-montgomery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbbbff57e50>]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 799.992x599.976 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_tr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
